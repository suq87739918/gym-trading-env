"""
å¢å¼ºè®­ç»ƒå™¨æ¨¡å—
æ”¯æŒLSTM PPOã€DQNä»¥åŠå…ˆè¿›çš„è®­ç»ƒç›‘æ§å’Œæ¨¡å‹å¯¹æ¯”åŠŸèƒ½
"""
import os
import time
import json
import numpy as np
import pandas as pd
import logging
import torch
from typing import Dict, List, Any, Optional, Tuple, Union
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Stable Baselines3 imports
from stable_baselines3 import PPO, DQN
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.callbacks import (
    BaseCallback, EvalCallback, CheckpointCallback
)
from stable_baselines3.common.policies import BasePolicy
from stable_baselines3.common.monitor import Monitor

# RecurrentPPO for LSTM support (if available)
try:
    from sb3_contrib import RecurrentPPO
    RECURRENT_PPO_AVAILABLE = True
except ImportError:
    RECURRENT_PPO_AVAILABLE = False
    print("âš ï¸ Warning: sb3_contrib not available, using standard PPO with LSTM policy")

from utils.config import get_config
from utils.logger import get_logger
from environment.trading_env import SolUsdtTradingEnv
from data.data_splitter import TimeSeriesDataSplitter

class PerformanceTracker:
    """æ€§èƒ½è·Ÿè¸ªå™¨"""
    
    def __init__(self, window_size: int = 100):
        self.window_size = window_size
        self.metrics_history = {
            'win_rate': [],
            'total_return': [],
            'sharpe_ratio': [],
            'max_drawdown': [],
            'avg_trade_duration': [],
            'profit_factor': [],
            'episode_rewards': [],
            'episode_lengths': []
        }
        self.best_metrics = {
            'win_rate': 0.0,
            'total_return': -np.inf,
            'sharpe_ratio': -np.inf,
            'max_drawdown': np.inf,
            'timestep': 0,
            'model_path': None
        }
        
    def update(self, metrics: Dict[str, float], timestep: int):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        for key, value in metrics.items():
            if key in self.metrics_history:
                self.metrics_history[key].append(value)
                # ä¿æŒçª—å£å¤§å°
                if len(self.metrics_history[key]) > self.window_size:
                    self.metrics_history[key].pop(0)
        
        # æ›´æ–°æœ€ä½³æŒ‡æ ‡
        if metrics.get('win_rate', 0) > self.best_metrics['win_rate']:
            self.best_metrics['win_rate'] = metrics['win_rate']
            self.best_metrics['timestep'] = timestep
        
        if metrics.get('total_return', -np.inf) > self.best_metrics['total_return']:
            self.best_metrics['total_return'] = metrics['total_return']
        
        if metrics.get('sharpe_ratio', -np.inf) > self.best_metrics['sharpe_ratio']:
            self.best_metrics['sharpe_ratio'] = metrics['sharpe_ratio']
        
        if metrics.get('max_drawdown', np.inf) < self.best_metrics['max_drawdown']:
            self.best_metrics['max_drawdown'] = metrics['max_drawdown']
    
    def get_recent_performance(self) -> Dict[str, float]:
        """è·å–æœ€è¿‘çš„æ€§èƒ½è¡¨ç°"""
        recent_metrics = {}
        for key, values in self.metrics_history.items():
            if values:
                recent_metrics[f'recent_{key}'] = np.mean(values[-10:])  # æœ€è¿‘10ä¸ªå€¼çš„å¹³å‡
                recent_metrics[f'latest_{key}'] = values[-1]
        return recent_metrics
    
    def is_performance_improving(self) -> bool:
        """åˆ¤æ–­æ€§èƒ½æ˜¯å¦åœ¨æ”¹å–„"""
        if len(self.metrics_history['win_rate']) < 20:
            return True  # æ•°æ®ä¸è¶³ï¼Œç»§ç»­è®­ç»ƒ
        
        recent_win_rate = np.mean(self.metrics_history['win_rate'][-10:])
        early_win_rate = np.mean(self.metrics_history['win_rate'][-20:-10])
        
        return recent_win_rate > early_win_rate

class EnhancedTrainingCallback(BaseCallback):
    """å¢å¼ºè®­ç»ƒå›è°ƒï¼Œæ”¯æŒé«˜çº§ç›‘æ§å’Œä¿å­˜ç­–ç•¥"""
    
    def __init__(self, 
                 eval_env,
                 performance_tracker: PerformanceTracker,
                 config: Dict,
                 save_path: str = "./models/best/",
                 verbose: int = 1):
        super().__init__(verbose)
        self.eval_env = eval_env
        self.performance_tracker = performance_tracker
        self.config = config
        self.save_path = save_path
        self.last_eval_step = 0
        self.last_save_step = 0
        self.training_start_time = time.time()
        
        # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        os.makedirs(save_path, exist_ok=True)
        
        # ç›‘æ§é…ç½®
        self.monitor_config = config.get('TRAINING_MONITOR', {})
        self.eval_freq = self.monitor_config.get('EVAL_FREQ', 25000)
        self.save_freq = self.monitor_config.get('SAVE_FREQ', 50000)
        self.log_interval = self.monitor_config.get('LOG_INTERVAL', 1000)
        
    def _on_step(self) -> bool:
        # å®šæœŸæ—¥å¿—è®°å½•
        if self.num_timesteps % self.log_interval == 0:
            self._log_training_progress()
        
        # å®šæœŸè¯„ä¼°
        if self.num_timesteps - self.last_eval_step >= self.eval_freq:
            self._evaluate_performance()
            self.last_eval_step = self.num_timesteps
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if self.num_timesteps - self.last_save_step >= self.save_freq:
            self._save_checkpoint()
            self.last_save_step = self.num_timesteps
        
        return True
    
    def _log_training_progress(self):
        """è®°å½•è®­ç»ƒè¿›åº¦"""
        elapsed_time = time.time() - self.training_start_time
        speed = self.num_timesteps / elapsed_time if elapsed_time > 0 else 0
        
        if self.verbose >= 1:
            print(f"ğŸ“Š æ­¥æ•°: {self.num_timesteps:,} | "
                  f"é€Ÿåº¦: {speed:.1f} æ­¥/ç§’ | "
                  f"ç”¨æ—¶: {elapsed_time/3600:.2f}h")
    
    def _evaluate_performance(self):
        """è¯„ä¼°å½“å‰æ¨¡å‹æ€§èƒ½"""
        try:
            # è¿è¡Œè¯„ä¼°ç¯å¢ƒ
            reset_result = self.eval_env.reset()
            
            # âœ… å…¼å®¹ä¸åŒç‰ˆæœ¬çš„gym/gymnasiumå’Œå‘é‡åŒ–ç¯å¢ƒ
            if isinstance(reset_result, tuple) and len(reset_result) == 2:
                # æ–°ç‰ˆæœ¬æ ¼å¼: (observation, info)
                obs, info = reset_result
            elif isinstance(reset_result, np.ndarray):
                # æ—§ç‰ˆæœ¬æ ¼å¼æˆ–å‘é‡åŒ–ç¯å¢ƒ: åªè¿”å›observation
                obs = reset_result
                info = {}
            else:
                # å…¶ä»–æƒ…å†µï¼Œå°è¯•è·å–è§‚æµ‹
                obs = reset_result
                info = {}
                
            episode_rewards = []
            episode_lengths = []
            
            eval_episodes = self.monitor_config.get('EVAL_EPISODES', 10)  # âœ… å‡å°‘è¯„ä¼°æ¬¡æ•°
            
            for episode in range(min(5, eval_episodes)):  # âœ… è¿›ä¸€æ­¥å‡å°‘è¯„ä¼°æ¬¡æ•°ä»¥èŠ‚çœæ—¶é—´
                episode_reward = 0
                episode_length = 0
                done = False
                
                while not done and episode_length < 500:  # âœ… å‡å°‘å•é›†é•¿åº¦é™åˆ¶
                    try:
                        action, _ = self.model.predict(obs, deterministic=True)
                        step_result = self.eval_env.step(action)
                        
                        # âœ… æ­£ç¡®å¤„ç†step()è¿”å›å€¼ - æ”¯æŒæ–°æ—§APIæ ¼å¼
                        if len(step_result) == 5:
                            obs, reward, terminated, truncated, info = step_result
                            done = terminated or truncated
                        elif len(step_result) == 4:
                            obs, reward, done, info = step_result
                        else:
                            self.logger.warning(f"âš ï¸ æ„å¤–çš„stepè¿”å›å€¼æ•°é‡: {len(step_result)}")
                            break
                        
                        # âœ… å®‰å…¨å¤„ç†rewardï¼ˆå¯èƒ½æ˜¯æ•°ç»„æˆ–æ ‡é‡ï¼‰
                        if isinstance(reward, np.ndarray):
                            episode_reward += reward[0] if len(reward) > 0 else 0
                        else:
                            episode_reward += reward
                            
                        episode_length += 1
                        
                        # âœ… å®‰å…¨å¤„ç†doneï¼ˆå¯èƒ½æ˜¯æ•°ç»„æˆ–æ ‡é‡ï¼‰
                        if isinstance(done, np.ndarray):
                            done = done[0] if len(done) > 0 else False
                            
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ è¯„ä¼°æ­¥éª¤å‡ºé”™: {e}")
                        break
                
                episode_rewards.append(episode_reward)
                episode_lengths.append(episode_length)
                
                # âœ… ä¸ºä¸‹ä¸€è½®é‡ç½®ç¯å¢ƒ
                try:
                    reset_result = self.eval_env.reset()
                    # âœ… å…¼å®¹ä¸åŒç‰ˆæœ¬çš„gym/gymnasiumå’Œå‘é‡åŒ–ç¯å¢ƒ
                    if isinstance(reset_result, tuple) and len(reset_result) == 2:
                        # æ–°ç‰ˆæœ¬æ ¼å¼: (observation, info)
                        obs, info = reset_result
                    elif isinstance(reset_result, np.ndarray):
                        # æ—§ç‰ˆæœ¬æ ¼å¼æˆ–å‘é‡åŒ–ç¯å¢ƒ: åªè¿”å›observation
                        obs = reset_result
                        info = {}
                    else:
                        # å…¶ä»–æƒ…å†µï¼Œå°è¯•è·å–è§‚æµ‹
                        obs = reset_result
                        info = {}
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ç¯å¢ƒé‡ç½®å¤±è´¥: {e}")
                    break
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            if episode_rewards:
                avg_reward = np.mean(episode_rewards)
                avg_length = np.mean(episode_lengths)
                
                # âœ… å®‰å…¨è·å–äº¤æ˜“ç¯å¢ƒçš„è¯¦ç»†ç»Ÿè®¡
                metrics = {
                    'avg_reward': avg_reward,
                    'avg_length': avg_length,
                    'win_rate': 0.5,  # é»˜è®¤å€¼
                    'total_return': avg_reward / 1000,  # ç²—ç•¥ä¼°ç®—
                    'sharpe_ratio': avg_reward / (np.std(episode_rewards) + 1e-8),
                    'max_drawdown': 0.1  # é»˜è®¤å€¼
                }
                
                # âœ… å°è¯•è·å–è¯¦ç»†çš„äº¤æ˜“ç»Ÿè®¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                try:
                    if hasattr(self.eval_env, 'envs') and len(self.eval_env.envs) > 0:
                        env = self.eval_env.envs[0]
                        if hasattr(env, 'get_trade_summary'):
                            trade_summary = env.get_trade_summary()
                            metrics.update({
                                'win_rate': trade_summary.get('win_rate', metrics['win_rate']),
                                'total_return': trade_summary.get('total_return', metrics['total_return']),
                                'sharpe_ratio': trade_summary.get('sharpe_ratio', metrics['sharpe_ratio']),
                                'max_drawdown': trade_summary.get('max_drawdown', metrics['max_drawdown'])
                            })
                except Exception as e:
                    self.logger.debug(f"ğŸ“Š æ— æ³•è·å–è¯¦ç»†äº¤æ˜“ç»Ÿè®¡: {e}")
                
                # æ›´æ–°æ€§èƒ½è¿½è¸ªå™¨
                self.performance_tracker.update(metrics, self.num_timesteps)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
                if self._is_best_model(metrics):
                    self._save_best_model(metrics)
                
                if self.verbose >= 1:
                    print(f"ğŸ“ˆ è¯„ä¼°ç»“æœ - å¹³å‡å¥–åŠ±: {avg_reward:.3f}, "
                          f"èƒœç‡: {metrics['win_rate']:.3f}, "
                          f"å¤æ™®æ¯”ç‡: {metrics['sharpe_ratio']:.3f}")
            else:
                self.logger.warning("âš ï¸ è¯„ä¼°æ²¡æœ‰äº§ç”Ÿæœ‰æ•ˆç»“æœ")
                
        except Exception as e:
            self.logger.error(f"âŒ è¯„ä¼°æ€§èƒ½å¤±è´¥: {e}")
            # ç»§ç»­è®­ç»ƒï¼Œä¸ä¸­æ–­
    
    def _is_best_model(self, metrics: Dict[str, float]) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹"""
        metric_name = self.monitor_config.get('BEST_MODEL_METRIC', 'win_rate')
        
        if metric_name == 'win_rate':
            threshold = self.monitor_config.get('WIN_RATE_THRESHOLD', 0.55)
            return metrics.get('win_rate', 0) >= threshold
        elif metric_name == 'total_return':
            threshold = self.monitor_config.get('MIN_TOTAL_RETURN', 0.10)
            return metrics.get('total_return', 0) >= threshold
        elif metric_name == 'sharpe_ratio':
            return metrics.get('sharpe_ratio', -np.inf) > self.performance_tracker.best_metrics['sharpe_ratio']
        
        return False
    
    def _save_best_model(self, metrics: Dict[str, float]):
        """ä¿å­˜æœ€ä½³æ¨¡å‹"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            model_filename = f"best_model_{timestamp}_step{self.num_timesteps}.zip"
            model_path = os.path.join(self.save_path, model_filename)
            
            self.model.save(model_path)
            self.performance_tracker.best_metrics['model_path'] = model_path
            
            # ä¿å­˜æ¨¡å‹è¯„ä¼°ä¿¡æ¯
            eval_info = {
                'timestep': self.num_timesteps,
                'metrics': metrics,
                'timestamp': timestamp,
                'model_path': model_path
            }
            
            info_path = os.path.join(self.save_path, f"best_model_info_{timestamp}.json")
            with open(info_path, 'w') as f:
                json.dump(eval_info, f, indent=2)
            
            print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {model_path}")
            print(f"ğŸ† èƒœç‡: {metrics['win_rate']:.3f} | æ”¶ç›Š: {metrics['total_return']:.3f}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æœ€ä½³æ¨¡å‹å¤±è´¥: {e}")
    
    def _save_checkpoint(self):
        """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹"""
        try:
            checkpoint_path = os.path.join(self.save_path, f"checkpoint_{self.num_timesteps}.zip")
            self.model.save(checkpoint_path)
            
            if self.verbose >= 1:
                print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")
                
        except Exception as e:
            print(f"âŒ ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")

class EnhancedTrainer:
    """å¢å¼ºè®­ç»ƒå™¨ - æ”¯æŒå¤šç§ç®—æ³•å’Œé«˜çº§ç›‘æ§"""
    
    def __init__(self):
        self.config = get_config()
        self.logger = get_logger('EnhancedTrainer', 'enhanced_trainer.log')
        self.performance_tracker = PerformanceTracker()
        
        # æ¨¡å‹é…ç½®
        self.model_configs = self.config.get('MODEL_CONFIGS', {})
        self.active_model = self.config.get('ACTIVE_MODEL', 'PPO_LSTM')
        
        # è®­ç»ƒç»“æœå­˜å‚¨
        self.training_results = {}
        
    def prepare_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        self.logger.info("ğŸ”„ å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        # ä½¿ç”¨æ—¶åºæ•°æ®åˆ’åˆ†å™¨
        splitter = TimeSeriesDataSplitter()
        train_df, val_df, test_df = splitter.split_data(df)
        
        # ç§»é™¤æ•°æ®é›†æ ‡è¯†åˆ—
        for dataset in [train_df, val_df, test_df]:
            if 'dataset_type' in dataset.columns:
                dataset.drop('dataset_type', axis=1, inplace=True)
        
        self.logger.info(f"ğŸ“š è®­ç»ƒé›†: {len(train_df):,} æ¡è®°å½•")
        self.logger.info(f"ğŸ“‹ éªŒè¯é›†: {len(val_df):,} æ¡è®°å½•")
        self.logger.info(f"ğŸ§ª æµ‹è¯•é›†: {len(test_df):,} æ¡è®°å½•")
        
        return train_df, val_df, test_df
    
    def create_environments(self, train_df: pd.DataFrame, val_df: pd.DataFrame) -> Tuple[Any, Any]:
        """åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯ç¯å¢ƒ"""
        self.logger.info("ğŸ—ï¸ åˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
        
        def make_env(data, mode='train'):
            def _init():
                try:
                    env = SolUsdtTradingEnv(data, mode=mode)
                    # âœ… ç®€åŒ–ç¯å¢ƒåˆ›å»ºï¼Œå»æ‰å¯èƒ½æœ‰é—®é¢˜çš„MonitoråŒ…è£…
                    return env
                except Exception as e:
                    self.logger.error(f"âŒ ç¯å¢ƒåˆ›å»ºå¤±è´¥: {e}")
                    raise e
            return _init
        
        # åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ
        n_envs = self.config.get('N_ENVS', 1)  # âœ… å‡å°‘åˆ°1ä¸ªç¯å¢ƒé¿å…å¤æ‚æ€§
        
        try:
            train_env = DummyVecEnv([make_env(train_df, 'train') for _ in range(n_envs)])
            train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True)
            
            # éªŒè¯ç¯å¢ƒ
            eval_env = DummyVecEnv([make_env(val_df, 'eval')])
            eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=True, training=False)
            
            self.logger.info(f"âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸ - è®­ç»ƒç¯å¢ƒæ•°é‡: {n_envs}")
            
            # âœ… æ·»åŠ ç¯å¢ƒæµ‹è¯•
            self.logger.info("ğŸ§ª æµ‹è¯•ç¯å¢ƒé‡ç½®...")
            try:
                reset_result = train_env.reset()
                
                # âœ… å…¼å®¹ä¸åŒç‰ˆæœ¬çš„gym/gymnasiumå’Œå‘é‡åŒ–ç¯å¢ƒ
                if isinstance(reset_result, tuple) and len(reset_result) == 2:
                    # æ–°ç‰ˆæœ¬æ ¼å¼: (observation, info)
                    test_obs, test_info = reset_result
                    self.logger.info(f"âœ… ç¯å¢ƒé‡ç½®æˆåŠŸï¼Œè§‚æµ‹å½¢çŠ¶: {test_obs.shape}")
                elif isinstance(reset_result, np.ndarray):
                    # æ—§ç‰ˆæœ¬æ ¼å¼æˆ–å‘é‡åŒ–ç¯å¢ƒ: åªè¿”å›observation
                    test_obs = reset_result
                    self.logger.info(f"âœ… ç¯å¢ƒé‡ç½®æˆåŠŸï¼Œè§‚æµ‹å½¢çŠ¶: {test_obs.shape}")
                else:
                    # å…¶ä»–æƒ…å†µï¼Œå°è¯•è·å–è§‚æµ‹
                    test_obs = reset_result
                    self.logger.info(f"âœ… ç¯å¢ƒé‡ç½®æˆåŠŸï¼Œè§‚æµ‹ç±»å‹: {type(test_obs)}")
                    
            except Exception as e:
                self.logger.error(f"âŒ ç¯å¢ƒé‡ç½®å¤±è´¥: {e}")
                # âœ… æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                import traceback
                self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                raise e
            
            return train_env, eval_env
            
        except Exception as e:
            self.logger.error(f"âŒ å‘é‡åŒ–ç¯å¢ƒåˆ›å»ºå¤±è´¥: {e}")
            raise e
    
    def train_ppo_lstm(self, train_env, eval_env, total_timesteps: int) -> str:
        """è®­ç»ƒLSTM PPOæ¨¡å‹"""
        self.logger.info("ğŸ§  å¼€å§‹è®­ç»ƒ LSTM PPO æ¨¡å‹...")
        
        model_config = self.model_configs.get('PPO_LSTM', {})
        
        try:
            if RECURRENT_PPO_AVAILABLE:
                # ä½¿ç”¨RecurrentPPO
                model = RecurrentPPO(
                    policy=model_config.get('policy', 'MlpLstmPolicy'),
                    env=train_env,
                    learning_rate=model_config.get('learning_rate', 1e-4),
                    gamma=model_config.get('gamma', 0.95),
                    gae_lambda=model_config.get('gae_lambda', 0.9),
                    clip_range=model_config.get('clip_range', 0.2),
                    n_steps=model_config.get('n_steps', 2048),
                    batch_size=model_config.get('batch_size', 256),
                    n_epochs=model_config.get('n_epochs', 10),
                    ent_coef=model_config.get('ent_coef', 0.005),
                    vf_coef=model_config.get('vf_coef', 0.5),
                    max_grad_norm=model_config.get('max_grad_norm', 0.5),
                    policy_kwargs=model_config.get('policy_kwargs', {}),
                    verbose=model_config.get('verbose', 1)
                )
            else:
                # ä½¿ç”¨æ ‡å‡†PPO with LSTM policy
                model = PPO(
                    policy='MlpLstmPolicy',
                    env=train_env,
                    learning_rate=model_config.get('learning_rate', 1e-4),
                    gamma=model_config.get('gamma', 0.95),
                    gae_lambda=model_config.get('gae_lambda', 0.9),
                    clip_range=model_config.get('clip_range', 0.2),
                    n_steps=model_config.get('n_steps', 2048),
                    batch_size=model_config.get('batch_size', 256),
                    n_epochs=model_config.get('n_epochs', 10),
                    ent_coef=model_config.get('ent_coef', 0.005),
                    vf_coef=model_config.get('vf_coef', 0.5),
                    max_grad_norm=model_config.get('max_grad_norm', 0.5),
                    policy_kwargs=model_config.get('policy_kwargs', {
                        'net_arch': [64, 64],
                        'lstm_hidden_size': 64,
                        'n_lstm_layers': 1
                    }),
                    verbose=model_config.get('verbose', 1)
                )
        
        except Exception as e:
            self.logger.error(f"âŒ LSTM PPOæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.logger.info("ğŸ”„ å›é€€åˆ°æ ‡å‡†PPOæ¨¡å‹...")
            return self.train_ppo_standard(train_env, eval_env, total_timesteps)
        
        # åˆ›å»ºå›è°ƒ
        callback = EnhancedTrainingCallback(
            eval_env=eval_env,
            performance_tracker=self.performance_tracker,
            config=self.config.to_dict(),
            save_path="./models/ppo_lstm/"
        )
        
        # å¼€å§‹è®­ç»ƒ
        start_time = time.time()
        model.learn(total_timesteps=total_timesteps, callback=callback)
        training_time = time.time() - start_time
        
        # ä¿å­˜æ¨¡å‹
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_path = f"models/ppo_lstm_final_{timestamp}.zip"
        model.save(model_path)
        
        # ä¿å­˜ç¯å¢ƒå½’ä¸€åŒ–å‚æ•°
        train_env.save("models/ppo_lstm_vec_normalize.pkl")
        
        self.logger.info(f"âœ… LSTM PPOè®­ç»ƒå®Œæˆ - è€—æ—¶: {training_time/3600:.2f}å°æ—¶")
        self.logger.info(f"ğŸ’¾ æ¨¡å‹ä¿å­˜è‡³: {model_path}")
        
        return model_path
    
    def train_ppo_standard(self, train_env, eval_env, total_timesteps: int) -> str:
        """
        è®­ç»ƒæ ‡å‡†PPOæ¨¡å‹ - ä¼˜åŒ–ç‰ˆï¼Œæ”¯æŒå¿«é€Ÿæµ‹è¯•
        """
        self.logger.info("ğŸ§  å¼€å§‹è®­ç»ƒæ ‡å‡†PPOæ¨¡å‹...")
        
        try:
            # âœ… ä¼˜åŒ–çš„PPOé…ç½®ï¼Œé€‚åˆå¿«é€Ÿæµ‹è¯•
            ppo_config = {
                'policy': 'MlpPolicy',
                'learning_rate': 3e-4,
                'n_steps': 512,        # å‡å°‘æ­¥æ•°
                'batch_size': 64,      # è¾ƒå°çš„æ‰¹æ¬¡
                'n_epochs': 4,         # å‡å°‘epoch
                'gamma': 0.99,
                'gae_lambda': 0.95,
                'clip_range': 0.2,
                'ent_coef': 0.01,
                'vf_coef': 0.5,
                'max_grad_norm': 0.5,
                'verbose': 1,
                'policy_kwargs': {
                    'net_arch': [128, 128],  # è¾ƒå°çš„ç½‘ç»œ
                    'activation_fn': torch.nn.ReLU
                }
            }
            
            self.logger.info("ğŸ”§ åˆ›å»ºPPOæ¨¡å‹...")
            
            # âœ… æ·»åŠ ç¯å¢ƒæµ‹è¯•
            self.logger.info("ğŸ§ª æµ‹è¯•ç¯å¢ƒé‡ç½®...")
            try:
                reset_result = train_env.reset()
                
                # âœ… å…¼å®¹ä¸åŒç‰ˆæœ¬çš„gym/gymnasiumå’Œå‘é‡åŒ–ç¯å¢ƒ
                if isinstance(reset_result, tuple) and len(reset_result) == 2:
                    # æ–°ç‰ˆæœ¬æ ¼å¼: (observation, info)
                    test_obs, test_info = reset_result
                    self.logger.info(f"âœ… ç¯å¢ƒé‡ç½®æˆåŠŸï¼Œè§‚æµ‹å½¢çŠ¶: {test_obs.shape}")
                elif isinstance(reset_result, np.ndarray):
                    # æ—§ç‰ˆæœ¬æ ¼å¼æˆ–å‘é‡åŒ–ç¯å¢ƒ: åªè¿”å›observation
                    test_obs = reset_result
                    self.logger.info(f"âœ… ç¯å¢ƒé‡ç½®æˆåŠŸï¼Œè§‚æµ‹å½¢çŠ¶: {test_obs.shape}")
                else:
                    # å…¶ä»–æƒ…å†µï¼Œå°è¯•è·å–è§‚æµ‹
                    test_obs = reset_result
                    self.logger.info(f"âœ… ç¯å¢ƒé‡ç½®æˆåŠŸï¼Œè§‚æµ‹ç±»å‹: {type(test_obs)}")
                    
            except Exception as e:
                self.logger.error(f"âŒ ç¯å¢ƒé‡ç½®å¤±è´¥: {e}")
                # âœ… æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                import traceback
                self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                raise e
            
            # åˆ›å»ºæ¨¡å‹
            self.logger.info("ğŸ”¨ åˆå§‹åŒ–PPOæ¨¡å‹...")
            model = PPO(env=train_env, **ppo_config)
            self.logger.info("âœ… PPOæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            
            # âœ… ç®€åŒ–å›è°ƒå‡½æ•°ï¼Œé¿å…å¤æ‚çš„è¯„ä¼°é€»è¾‘
            performance_tracker = PerformanceTracker(window_size=50)
            
            # âœ… ä½¿ç”¨æ›´ç®€å•çš„å›è°ƒæˆ–ä¸ä½¿ç”¨å›è°ƒ
            self.logger.info("ğŸ¯ å¼€å§‹è®­ç»ƒï¼ˆæ— å›è°ƒæ¨¡å¼ï¼‰...")
            
            # âœ… å…ˆå°è¯•æ— å›è°ƒè®­ç»ƒ
            model.learn(
                total_timesteps=total_timesteps,
                reset_num_timesteps=False,
                progress_bar=True
            )
            
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            save_path = f"./models/ppo_standard_final_{int(time.time())}.zip"
            model.save(save_path)
            
            self.logger.info(f"âœ… PPOæ ‡å‡†æ¨¡å‹è®­ç»ƒå®Œæˆ")
            self.logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {save_path}")
            
            return save_path
            
        except Exception as e:
            self.logger.error(f"âŒ PPOæ ‡å‡†æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            self.logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return None
    
    def train_dqn(self, train_env, eval_env, total_timesteps: int) -> str:
        """è®­ç»ƒDQNæ¨¡å‹"""
        self.logger.info("ğŸ§  å¼€å§‹è®­ç»ƒDQNæ¨¡å‹...")
        
        model_config = self.model_configs.get('DQN', {})
        
        model = DQN(
            policy='MlpPolicy',
            env=train_env,
            learning_rate=model_config.get('learning_rate', 1e-4),
            buffer_size=model_config.get('buffer_size', 100000),
            learning_starts=model_config.get('learning_starts', 50000),
            batch_size=model_config.get('batch_size', 128),
            tau=model_config.get('tau', 1.0),
            gamma=model_config.get('gamma', 0.95),
            train_freq=model_config.get('train_freq', 4),
            gradient_steps=model_config.get('gradient_steps', 1),
            target_update_interval=model_config.get('target_update_interval', 10000),
            exploration_fraction=model_config.get('exploration_fraction', 0.1),
            exploration_initial_eps=model_config.get('exploration_initial_eps', 1.0),
            exploration_final_eps=model_config.get('exploration_final_eps', 0.05),
            policy_kwargs=model_config.get('policy_kwargs', {'net_arch': [128, 128, 64]}),
            verbose=model_config.get('verbose', 1)
        )
        
        # åˆ›å»ºå›è°ƒ
        callback = EnhancedTrainingCallback(
            eval_env=eval_env,
            performance_tracker=self.performance_tracker,
            config=self.config.to_dict(),
            save_path="./models/dqn/"
        )
        
        # å¼€å§‹è®­ç»ƒ
        start_time = time.time()
        model.learn(total_timesteps=total_timesteps, callback=callback)
        training_time = time.time() - start_time
        
        # ä¿å­˜æ¨¡å‹
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_path = f"models/dqn_final_{timestamp}.zip"
        model.save(model_path)
        
        # ä¿å­˜ç¯å¢ƒå½’ä¸€åŒ–å‚æ•°
        train_env.save("models/dqn_vec_normalize.pkl")
        
        self.logger.info(f"âœ… DQNè®­ç»ƒå®Œæˆ - è€—æ—¶: {training_time/3600:.2f}å°æ—¶")
        self.logger.info(f"ğŸ’¾ æ¨¡å‹ä¿å­˜è‡³: {model_path}")
        
        return model_path
    
    def train_multiple_models(self, df: pd.DataFrame) -> Dict[str, str]:
        """è®­ç»ƒå¤šä¸ªæ¨¡å‹å¹¶å¯¹æ¯”æ€§èƒ½"""
        self.logger.info("ğŸš€ å¼€å§‹å¤šæ¨¡å‹è®­ç»ƒå’Œå¯¹æ¯”...")
        
        # å‡†å¤‡æ•°æ®
        train_df, val_df, test_df = self.prepare_data(df)
        
        # åˆ›å»ºç¯å¢ƒ
        train_env, eval_env = self.create_environments(train_df, val_df)
        
        # è·å–è®­ç»ƒæ­¥æ•°
        total_timesteps = self.config.get('TOTAL_TIMESTEPS', 500000)
        
        # è®­ç»ƒé…ç½®
        models_to_train = ['PPO_LSTM', 'PPO_STANDARD', 'DQN'] if self.config.get('TRAINING_MONITOR', {}).get('MODEL_COMPARISON', True) else [self.active_model]
        
        trained_models = {}
        
        for model_name in models_to_train:
            try:
                self.logger.info(f"\n{'='*60}")
                self.logger.info(f"ğŸ¯ å¼€å§‹è®­ç»ƒ {model_name} æ¨¡å‹")
                self.logger.info(f"{'='*60}")
                
                # é‡ç½®æ€§èƒ½è¿½è¸ªå™¨
                self.performance_tracker = PerformanceTracker()
                
                if model_name == 'PPO_LSTM':
                    model_path = self.train_ppo_lstm(train_env, eval_env, total_timesteps)
                elif model_name == 'PPO_STANDARD':
                    model_path = self.train_ppo_standard(train_env, eval_env, total_timesteps)
                elif model_name == 'DQN':
                    model_path = self.train_dqn(train_env, eval_env, total_timesteps)
                else:
                    self.logger.warning(f"âš ï¸ æœªçŸ¥æ¨¡å‹ç±»å‹: {model_name}")
                    continue
                
                trained_models[model_name] = model_path
                
                # ä¿å­˜æ€§èƒ½ç»“æœ
                self.training_results[model_name] = {
                    'model_path': model_path,
                    'best_metrics': self.performance_tracker.best_metrics.copy(),
                    'final_performance': self.performance_tracker.get_recent_performance()
                }
                
                self.logger.info(f"âœ… {model_name} è®­ç»ƒå®Œæˆ")
                
            except Exception as e:
                self.logger.error(f"âŒ {model_name} è®­ç»ƒå¤±è´¥: {e}")
                continue
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        self._generate_comparison_report()
        
        # ä¿å­˜æµ‹è¯•æ•°æ®ä¾›å›æµ‹ä½¿ç”¨
        test_data_path = "data/test_data.pkl"
        test_df.to_pickle(test_data_path)
        self.logger.info(f"ğŸ’¾ æµ‹è¯•æ•°æ®å·²ä¿å­˜è‡³: {test_data_path}")
        
        return trained_models
    
    def _generate_comparison_report(self):
        """ç”Ÿæˆæ¨¡å‹å¯¹æ¯”æŠ¥å‘Š"""
        if not self.training_results:
            return
        
        self.logger.info("\n" + "="*80)
        self.logger.info("ğŸ“Š æ¨¡å‹è®­ç»ƒå¯¹æ¯”æŠ¥å‘Š")
        self.logger.info("="*80)
        
        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
        comparison_data = []
        for model_name, results in self.training_results.items():
            best_metrics = results['best_metrics']
            comparison_data.append({
                'Model': model_name,
                'Win Rate': f"{best_metrics.get('win_rate', 0):.3f}",
                'Total Return': f"{best_metrics.get('total_return', 0):.3f}",
                'Sharpe Ratio': f"{best_metrics.get('sharpe_ratio', 0):.3f}",
                'Max Drawdown': f"{best_metrics.get('max_drawdown', 0):.3f}",
                'Best Timestep': best_metrics.get('timestep', 0),
                'Model Path': results['model_path']
            })
        
        # æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
        if comparison_data:
            df_comparison = pd.DataFrame(comparison_data)
            self.logger.info("\næ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
            self.logger.info(df_comparison.to_string(index=False))
            
            # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
            best_model = max(comparison_data, key=lambda x: float(x['Win Rate']))
            self.logger.info(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model['Model']}")
            self.logger.info(f"   èƒœç‡: {best_model['Win Rate']}")
            self.logger.info(f"   æ€»æ”¶ç›Š: {best_model['Total Return']}")
            self.logger.info(f"   æ¨¡å‹è·¯å¾„: {best_model['Model Path']}")
            
            # ä¿å­˜å¯¹æ¯”æŠ¥å‘Š
            report_path = f"results/model_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            os.makedirs(os.path.dirname(report_path), exist_ok=True)
            df_comparison.to_csv(report_path, index=False)
            self.logger.info(f"ğŸ“‹ å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        self.logger.info("="*80)

    def _quick_model_test(self, model_path: str, test_df: pd.DataFrame) -> Dict:
        """
        å¿«é€Ÿæ¨¡å‹æµ‹è¯• - ç”¨äºéªŒè¯è®­ç»ƒæ•ˆæœ
        """
        try:
            from stable_baselines3 import PPO
            import numpy as np
            
            # åŠ è½½æ¨¡å‹
            model = PPO.load(model_path)
            
            # åˆ›å»ºæµ‹è¯•ç¯å¢ƒ
            from environment.trading_env import SolUsdtTradingEnv
            test_env = SolUsdtTradingEnv(test_df, mode='test')
            
            # è¿è¡Œä¸€ä¸ªå›åˆ
            reset_result = test_env.reset()
            
            # âœ… å…¼å®¹ä¸åŒç‰ˆæœ¬çš„gym/gymnasium
            if isinstance(reset_result, tuple) and len(reset_result) == 2:
                # æ–°ç‰ˆæœ¬æ ¼å¼: (observation, info)
                obs, info = reset_result
            elif isinstance(reset_result, np.ndarray):
                # æ—§ç‰ˆæœ¬æ ¼å¼: åªè¿”å›observation
                obs = reset_result
                info = {}
            else:
                # å…¶ä»–æƒ…å†µï¼Œå°è¯•è·å–è§‚æµ‹
                obs = reset_result
                info = {}
                
            total_reward = 0
            step_count = 0
            max_steps = min(len(test_df) - 100, 5000)  # ğŸ”§ å¢åŠ æœ€å¤§æ­¥æ•°ï¼Œä½†ä¸è¶…è¿‡æ•°æ®é•¿åº¦
            
            self.logger.info(f"ğŸ“Š å¼€å§‹å¿«é€Ÿæµ‹è¯•ï¼Œæœ€å¤§æ­¥æ•°: {max_steps}")
            
            while True:
                action, _ = model.predict(obs, deterministic=True)
                step_result = test_env.step(action)
                
                # âœ… æ­£ç¡®å¤„ç†step()è¿”å›çš„5ä¸ªå€¼ (obs, reward, terminated, truncated, info)
                if len(step_result) == 5:
                    obs, reward, terminated, truncated, info = step_result
                    done = terminated or truncated
                else:
                    # å‘åå…¼å®¹æ€§å¤„ç†
                    obs, reward, done, info = step_result
                    
                total_reward += reward
                step_count += 1
                
                # ğŸ”§ æ¯1000æ­¥è¾“å‡ºè¿›åº¦
                if step_count % 1000 == 0:
                    current_portfolio = info.get('portfolio_value', test_env.initial_balance)
                    current_return = (current_portfolio - test_env.initial_balance) / test_env.initial_balance
                    self.logger.info(f"ğŸ“ˆ æ­¥æ•°: {step_count}, å½“å‰æ”¶ç›Šç‡: {current_return:.2%}, æŠ•èµ„ç»„åˆä»·å€¼: ${current_portfolio:.2f}")
                
                if done or step_count >= max_steps:
                    break
            
            # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
            portfolio_value = info.get('portfolio_value', test_env.initial_balance)
            total_return = (portfolio_value - test_env.initial_balance) / test_env.initial_balance
            
            # è®¡ç®—ç®€å•å¤æ™®æ¯”ç‡ï¼ˆå¦‚æœæœ‰è¶³å¤Ÿæ•°æ®ï¼‰
            if len(test_env.portfolio_history) > 10:
                returns = np.diff(test_env.portfolio_history) / test_env.portfolio_history[:-1]
                sharpe_ratio = np.mean(returns) / (np.std(returns) + 1e-8) * np.sqrt(252)
            else:
                sharpe_ratio = 0.0
            
            results = {
                'total_return': total_return,
                'sharpe_ratio': sharpe_ratio,
                'total_trades': info.get('total_trades', 0),
                'win_rate': info.get('win_rate', 0),
                'max_drawdown': info.get('max_drawdown', 0),
                'steps': step_count,
                'portfolio_value': portfolio_value,
                'initial_balance': test_env.initial_balance,
                'total_reward': total_reward
            }
            
            self.logger.info(f"âœ… å¿«é€Ÿæµ‹è¯•å®Œæˆ: æ”¶ç›Šç‡={total_return:.2%}, å¤æ™®æ¯”ç‡={sharpe_ratio:.3f}")
            self.logger.info(f"ğŸ“Š æµ‹è¯•è¯¦æƒ…: æ­¥æ•°={step_count}/{max_steps}, äº¤æ˜“æ¬¡æ•°={info.get('total_trades', 0)}, èƒœç‡={info.get('win_rate', 0):.2%}")
            self.logger.info(f"ğŸ’° èµ„äº§å˜åŒ–: ${test_env.initial_balance:.2f} â†’ ${portfolio_value:.2f}")
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ å¿«é€Ÿæµ‹è¯•å¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return None

def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•å¢å¼ºè®­ç»ƒå™¨"""
    from data.data_collector import DataCollector
    
    print("ğŸš€ æµ‹è¯•å¢å¼ºè®­ç»ƒå™¨")
    
    # åŠ è½½æ•°æ®
    collector = DataCollector()
    df = collector.load_data()
    
    if df.empty:
        print("âŒ æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®")
        return
    
    # åˆ›å»ºå¢å¼ºè®­ç»ƒå™¨
    trainer = EnhancedTrainer()
    
    # è®­ç»ƒå¤šä¸ªæ¨¡å‹
    trained_models = trainer.train_multiple_models(df)
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼Œå…±è®­ç»ƒ {len(trained_models)} ä¸ªæ¨¡å‹:")
    for model_name, model_path in trained_models.items():
        print(f"  {model_name}: {model_path}")

if __name__ == "__main__":
    main() 