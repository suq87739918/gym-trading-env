"""
å¹³è¡¡å¥–åŠ±å‡½æ•°æ¨¡å— - å®ç°é«˜èƒœç‡ä¸é«˜å¹´åŒ–æ”¶ç›Šçš„å¤šç›®æ ‡ä¼˜åŒ–
åŸºäºæ•°å­¦å…¬å¼: r_t = R_t + Î±Â·1{R_t>0} - Î²Â·1{R_t<0} - Î³Â·DD_t

æ ¸å¿ƒè®¾è®¡åŸç†ï¼š
1. R_t: å½“æ­¥ç›¸å¯¹æ”¶ç›Šç‡ï¼ˆåŸºç¡€æ”¶ç›Šé¡¹ï¼‰
2. Î±Â·1{R_t>0}: èƒœç‡å¥–åŠ±é¡¹ï¼ˆç›ˆåˆ©æ—¶çš„å›ºå®šå¥–åŠ±ï¼‰
3. Î²Â·1{R_t<0}: äºæŸæƒ©ç½šé¡¹ï¼ˆäºæŸæ—¶çš„å›ºå®šæƒ©ç½šï¼‰
4. Î³Â·DD_t: å›æ’¤è½¯çº¦æŸé¡¹ï¼ˆå½“å‰å›æ’¤çš„æƒ©ç½šï¼‰

å¤šç›®æ ‡å¹³è¡¡ï¼š
- å¹´åŒ–æ”¶ç›Šæœ€å¤§åŒ–ï¼šé€šè¿‡R_té¡¹ç›´æ¥æ¿€åŠ±æ”¶ç›Š
- èƒœç‡æå‡ï¼šé€šè¿‡Î±, Î²å‚æ•°è°ƒèŠ‚èƒœè´Ÿå¥–åŠ±å¼ºåº¦
- é£é™©æ§åˆ¶ï¼šé€šè¿‡Î³å‚æ•°æ§åˆ¶å›æ’¤å®¹å¿åº¦
"""

import numpy as np
import pandas as pd
from typing import Dict, Tuple, List, Optional
import logging
from dataclasses import dataclass
from enum import Enum

class RewardObjective(Enum):
    """å¥–åŠ±ç›®æ ‡ç±»å‹"""
    BALANCED = "balanced"           # å¹³è¡¡æ”¶ç›Šã€èƒœç‡ã€é£é™©
    HIGH_WINRATE = "high_winrate"   # é«˜èƒœç‡å¯¼å‘
    HIGH_RETURN = "high_return"     # é«˜æ”¶ç›Šå¯¼å‘
    LOW_RISK = "low_risk"          # ä½é£é™©å¯¼å‘
    ADAPTIVE = "adaptive"          # è‡ªé€‚åº”è°ƒæ•´

@dataclass
class BalancedRewardConfig:
    """å¹³è¡¡å¥–åŠ±å‡½æ•°é…ç½®å‚æ•°"""
    
    # ================= æ ¸å¿ƒæ•°å­¦å…¬å¼å‚æ•° =================
    # r_t = R_t + Î±Â·1{R_t>0} - Î²Â·1{R_t<0} - Î³Â·DD_t
    
    # èƒœç‡å¥–åŠ±å‚æ•° (Î±)
    alpha_small_win: float = 0.5    # å°ç›ˆåˆ©å¥–åŠ± (0-2%)
    alpha_medium_win: float = 1.0   # ä¸­ç›ˆåˆ©å¥–åŠ± (2-5%)
    alpha_large_win: float = 2.0    # å¤§ç›ˆåˆ©å¥–åŠ± (>5%)
    
    # äºæŸæƒ©ç½šå‚æ•° (Î²) - é€šå¸¸ Î² > Î± ä»¥æå‡èƒœç‡å€¾å‘
    beta_small_loss: float = 0.8    # å°äºæŸæƒ©ç½š (0-2%)
    beta_medium_loss: float = 1.8   # ä¸­äºæŸæƒ©ç½š (2-5%)
    beta_large_loss: float = 3.5    # å¤§äºæŸæƒ©ç½š (>5%)
    
    # å›æ’¤è½¯çº¦æŸå‚æ•° (Î³)
    gamma_drawdown: float = 2.0     # å›æ’¤æƒ©ç½šç³»æ•°
    
    # ================= æ”¶ç›Šè®¡ç®—å‚æ•° =================
    return_scale_factor: float = 100.0    # R_t ç¼©æ”¾å› å­
    use_log_returns: bool = False          # æ˜¯å¦ä½¿ç”¨å¯¹æ•°æ”¶ç›Š
    min_return_threshold: float = 0.001    # æœ€å°æ”¶ç›Šé˜ˆå€¼ï¼ˆé¿å…å™ªéŸ³ï¼‰
    
    # ================= èƒœç‡åˆ†çº§é˜ˆå€¼ =================
    small_profit_threshold: float = 0.02   # 2%
    medium_profit_threshold: float = 0.05  # 5%
    small_loss_threshold: float = 0.02     # 2%
    medium_loss_threshold: float = 0.05    # 5%
    
    # ================= å›æ’¤è®¡ç®—å‚æ•° =================
    drawdown_window: int = 100             # å›æ’¤è®¡ç®—çª—å£
    max_drawdown_penalty: float = 5.0      # æœ€å¤§å›æ’¤æƒ©ç½š
    drawdown_recovery_bonus: float = 0.5   # å›æ’¤æ¢å¤å¥–åŠ±
    
    # ================= å¹´åŒ–æ”¶ç›Šå¢å¼ºå‚æ•° =================
    annual_return_bonus_scale: float = 1.5  # å¹´åŒ–æ”¶ç›Šå¥–åŠ±å€æ•°
    sharpe_ratio_weight: float = 0.3        # å¤æ™®æ¯”ç‡æƒé‡
    profit_factor_weight: float = 0.2       # ç›ˆäºæ¯”æƒé‡
    
    # ================= è¿èƒœ/è¿è´¥è°ƒæ•´ =================
    consecutive_win_bonus: float = 0.1      # è¿èƒœå¥–åŠ±é€’å¢
    max_consecutive_bonus: float = 1.0      # è¿èƒœå¥–åŠ±ä¸Šé™
    consecutive_loss_penalty: float = 0.2   # è¿è´¥æƒ©ç½šé€’å¢
    max_consecutive_penalty: float = 2.0    # è¿è´¥æƒ©ç½šä¸Šé™
    
    # ================= è‡ªé€‚åº”è°ƒæ•´å‚æ•° =================
    enable_adaptive: bool = True            # å¯ç”¨è‡ªé€‚åº”è°ƒæ•´
    performance_window: int = 50            # è¡¨ç°è¯„ä¼°çª—å£
    alpha_adjustment_rate: float = 0.1      # Î±å‚æ•°è°ƒæ•´é€Ÿç‡
    beta_adjustment_rate: float = 0.1       # Î²å‚æ•°è°ƒæ•´é€Ÿç‡
    gamma_adjustment_rate: float = 0.05     # Î³å‚æ•°è°ƒæ•´é€Ÿç‡
    
    # ================= ç›®æ ‡æƒé‡é…ç½® =================
    winrate_target: float = 0.65            # ç›®æ ‡èƒœç‡
    annual_return_target: float = 0.30      # ç›®æ ‡å¹´åŒ–æ”¶ç›Šç‡
    max_drawdown_target: float = 0.15       # ç›®æ ‡æœ€å¤§å›æ’¤

class BalancedRewardFunction:
    """å¹³è¡¡å¥–åŠ±å‡½æ•°è®¡ç®—å™¨"""
    
    def __init__(self, config: BalancedRewardConfig = None, logger: logging.Logger = None):
        self.config = config or BalancedRewardConfig()
        self.logger = logger or self._setup_logger()
        
        # å†å²æ•°æ®å­˜å‚¨
        self.portfolio_history: List[float] = []
        self.return_history: List[float] = []
        self.reward_history: List[float] = []
        self.peak_portfolio_value: float = 0.0
        
        # äº¤æ˜“ç»Ÿè®¡
        self.total_trades: int = 0
        self.winning_trades: int = 0
        self.losing_trades: int = 0
        self.consecutive_wins: int = 0
        self.consecutive_losses: int = 0
        
        # åŠ¨æ€å‚æ•°ï¼ˆè‡ªé€‚åº”è°ƒæ•´ï¼‰
        self.current_alpha: float = self.config.alpha_medium_win
        self.current_beta: float = self.config.beta_medium_loss
        self.current_gamma: float = self.config.gamma_drawdown
        
        self.logger.info(f"ğŸ¯ å¹³è¡¡å¥–åŠ±å‡½æ•°åˆå§‹åŒ–: Î±={self.current_alpha:.2f}, "
                        f"Î²={self.current_beta:.2f}, Î³={self.current_gamma:.2f}")
    
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—å™¨"""
        logger = logging.getLogger(__name__)
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def calculate_reward(self, current_portfolio_value: float, previous_portfolio_value: float,
                        action: int, trade_completed: bool = False, 
                        trade_pnl_pct: float = None) -> Tuple[float, Dict]:
        """
        âœ… æ ¸å¿ƒå¥–åŠ±è®¡ç®—å‡½æ•°
        å®ç°æ•°å­¦å…¬å¼: r_t = R_t + Î±Â·1{R_t>0} - Î²Â·1{R_t<0} - Î³Â·DD_t
        
        Args:
            current_portfolio_value: å½“å‰ç»„åˆä»·å€¼
            previous_portfolio_value: å‰ä¸€æ­¥ç»„åˆä»·å€¼
            action: æ‰§è¡Œçš„åŠ¨ä½œ (0=æŒä»“, 1=å¼€å¤š, 2=å¼€ç©º, 3=å¹³ä»“)
            trade_completed: æ˜¯å¦å®Œæˆäº†ä¸€ç¬”äº¤æ˜“
            trade_pnl_pct: äº¤æ˜“ç›ˆäºç™¾åˆ†æ¯”ï¼ˆä»…åœ¨äº¤æ˜“å®Œæˆæ—¶æä¾›ï¼‰
            
        Returns:
            (æ€»å¥–åŠ±, å¥–åŠ±åˆ†è§£å­—å…¸)
        """
        
        # æ›´æ–°å†å²æ•°æ®
        self.portfolio_history.append(current_portfolio_value)
        self.peak_portfolio_value = max(self.peak_portfolio_value, current_portfolio_value)
        
        # å¥–åŠ±åˆ†è§£å­—å…¸
        reward_breakdown = {
            'R_t': 0.0,              # åŸºç¡€æ”¶ç›Šé¡¹
            'alpha_term': 0.0,       # èƒœç‡å¥–åŠ±é¡¹ Î±Â·1{R_t>0}
            'beta_term': 0.0,        # äºæŸæƒ©ç½šé¡¹ Î²Â·1{R_t<0}
            'gamma_term': 0.0,       # å›æ’¤è½¯çº¦æŸé¡¹ Î³Â·DD_t
            'annual_return_bonus': 0.0,  # å¹´åŒ–æ”¶ç›Šå¥–åŠ±
            'consecutive_bonus': 0.0,    # è¿èƒœ/è¿è´¥è°ƒæ•´
            'total_reward': 0.0      # æ€»å¥–åŠ±
        }
        
        # ==================== 1. è®¡ç®—åŸºç¡€æ”¶ç›Šé¡¹ R_t ====================
        if previous_portfolio_value > 0:
            if self.config.use_log_returns:
                # å¯¹æ•°æ”¶ç›Šï¼šr_t = ln(V_t / V_{t-1})
                R_t = np.log(current_portfolio_value / previous_portfolio_value)
            else:
                # ç›¸å¯¹æ”¶ç›Šï¼šR_t = (V_t - V_{t-1}) / V_{t-1}
                R_t = (current_portfolio_value - previous_portfolio_value) / previous_portfolio_value
            
            # åº”ç”¨ç¼©æ”¾å› å­å’Œé˜ˆå€¼è¿‡æ»¤
            if abs(R_t) >= self.config.min_return_threshold:
                R_t_scaled = R_t * self.config.return_scale_factor
            else:
                R_t_scaled = 0.0  # è¿‡æ»¤å°å¹…æ³¢åŠ¨å™ªéŸ³
        else:
            R_t = 0.0
            R_t_scaled = 0.0
        
        reward_breakdown['R_t'] = R_t_scaled
        self.return_history.append(R_t)
        
        # ==================== 2. è®¡ç®—èƒœç‡å¥–åŠ±/æƒ©ç½šé¡¹ ====================
        alpha_term = 0.0
        beta_term = 0.0
        
        if trade_completed and trade_pnl_pct is not None:
            # äº¤æ˜“å®Œæˆæ—¶ï¼ŒåŸºäºå®é™…äº¤æ˜“ç»“æœè®¡ç®—å¥–åŠ±/æƒ©ç½š
            self.total_trades += 1
            
            if trade_pnl_pct > 0:  # ç›ˆåˆ©äº¤æ˜“
                self.winning_trades += 1
                self.consecutive_wins += 1
                self.consecutive_losses = 0
                
                # æ ¹æ®ç›ˆåˆ©å¹…åº¦é€‰æ‹©Î±å€¼
                if trade_pnl_pct > self.config.medium_profit_threshold:  # >5%
                    alpha_term = self.config.alpha_large_win
                elif trade_pnl_pct > self.config.small_profit_threshold:  # 2%-5%
                    alpha_term = self.config.alpha_medium_win
                else:  # 0%-2%
                    alpha_term = self.config.alpha_small_win
                
                self.logger.debug(f"ğŸ’° ç›ˆåˆ©äº¤æ˜“: {trade_pnl_pct*100:.2f}%, Î±å¥–åŠ±={alpha_term:.2f}")
                
            else:  # äºæŸäº¤æ˜“
                self.losing_trades += 1
                self.consecutive_losses += 1
                self.consecutive_wins = 0
                
                # æ ¹æ®äºæŸå¹…åº¦é€‰æ‹©Î²å€¼
                abs_loss = abs(trade_pnl_pct)
                if abs_loss > self.config.medium_loss_threshold:  # >5%
                    beta_term = -self.config.beta_large_loss
                elif abs_loss > self.config.small_loss_threshold:  # 2%-5%
                    beta_term = -self.config.beta_medium_loss
                else:  # 0%-2%
                    beta_term = -self.config.beta_small_loss
                
                self.logger.debug(f"ğŸ“‰ äºæŸäº¤æ˜“: {trade_pnl_pct*100:.2f}%, Î²æƒ©ç½š={beta_term:.2f}")
        
        elif R_t != 0 and not trade_completed:
            # éäº¤æ˜“å®Œæˆæ—¶ï¼ŒåŸºäºå•æ­¥æ”¶ç›Šè®¡ç®—ï¼ˆè¾ƒå°çš„å¥–åŠ±/æƒ©ç½šï¼‰
            if R_t > 0:
                alpha_term = self.current_alpha * 0.1  # é™ä½éäº¤æ˜“æ—¶çš„å¥–åŠ±
            else:
                beta_term = -self.current_beta * 0.1   # é™ä½éäº¤æ˜“æ—¶çš„æƒ©ç½š
        
        reward_breakdown['alpha_term'] = alpha_term
        reward_breakdown['beta_term'] = beta_term
        
        # ==================== 3. è®¡ç®—å›æ’¤è½¯çº¦æŸé¡¹ Î³Â·DD_t ====================
        current_drawdown = self._calculate_current_drawdown()
        gamma_term = -self.current_gamma * current_drawdown
        
        # é™åˆ¶å›æ’¤æƒ©ç½šçš„æœ€å¤§å€¼
        gamma_term = max(gamma_term, -self.config.max_drawdown_penalty)
        
        reward_breakdown['gamma_term'] = gamma_term
        
        # ==================== 4. è®¡ç®—è¿èƒœ/è¿è´¥è°ƒæ•´ ====================
        consecutive_bonus = 0.0
        
        if self.consecutive_wins >= 3:
            # è¿èƒœå¥–åŠ±ï¼šé€’å¢ä½†æœ‰ä¸Šé™
            consecutive_bonus = min(
                self.consecutive_wins * self.config.consecutive_win_bonus,
                self.config.max_consecutive_bonus
            )
        elif self.consecutive_losses >= 3:
            # è¿è´¥æƒ©ç½šï¼šé€’å¢ä½†æœ‰ä¸Šé™
            consecutive_bonus = -min(
                self.consecutive_losses * self.config.consecutive_loss_penalty,
                self.config.max_consecutive_penalty
            )
        
        reward_breakdown['consecutive_bonus'] = consecutive_bonus
        
        # ==================== 5. è®¡ç®—å¹´åŒ–æ”¶ç›Šå¥–åŠ± ====================
        annual_return_bonus = 0.0
        if len(self.portfolio_history) >= 20:  # è¶³å¤Ÿçš„å†å²æ•°æ®
            annual_return_bonus = self._calculate_annual_return_bonus()
        
        reward_breakdown['annual_return_bonus'] = annual_return_bonus
        
        # ==================== 6. ç»„åˆæœ€ç»ˆå¥–åŠ± ====================
        total_reward = (R_t_scaled + alpha_term + beta_term + gamma_term + 
                       consecutive_bonus + annual_return_bonus)
        
        # é™åˆ¶å¥–åŠ±èŒƒå›´ï¼Œé¿å…æç«¯å€¼
        total_reward = np.clip(total_reward, -20.0, 20.0)
        reward_breakdown['total_reward'] = total_reward
        
        # æ›´æ–°å¥–åŠ±å†å²
        self.reward_history.append(total_reward)
        
        # ==================== 7. è‡ªé€‚åº”å‚æ•°è°ƒæ•´ ====================
        if self.config.enable_adaptive and self.total_trades % 10 == 0:
            self._adaptive_parameter_adjustment()
        
        # ==================== 8. è¯¦ç»†æ—¥å¿—è®°å½• ====================
        if trade_completed or abs(total_reward) > 1.0:
            self.logger.info(f"ğŸ å¥–åŠ±è®¡ç®—: R_t={R_t_scaled:.3f}, Î±={alpha_term:.3f}, "
                           f"Î²={beta_term:.3f}, Î³={gamma_term:.3f}, "
                           f"è¿èƒœ={consecutive_bonus:.3f}, å¹´åŒ–={annual_return_bonus:.3f}, "
                           f"æ€»å¥–åŠ±={total_reward:.3f}")
        
        return total_reward, reward_breakdown
    
    def _calculate_current_drawdown(self) -> float:
        """
        è®¡ç®—å½“å‰å›æ’¤ DD_t
        DD_t = (max(V_0...V_t) - V_t) / max(V_0...V_t)
        """
        if len(self.portfolio_history) == 0 or self.peak_portfolio_value <= 0:
            return 0.0
        
        current_value = self.portfolio_history[-1]
        drawdown = (self.peak_portfolio_value - current_value) / self.peak_portfolio_value
        
        return max(0.0, drawdown)  # ç¡®ä¿éè´Ÿ
    
    def _calculate_annual_return_bonus(self) -> float:
        """è®¡ç®—å¹´åŒ–æ”¶ç›Šå¥–åŠ±"""
        try:
            if len(self.portfolio_history) < 10:
                return 0.0
            
            # è®¡ç®—æœ€è¿‘ä¸€æ®µæ—¶é—´çš„æ”¶ç›Šç‡
            recent_window = min(252, len(self.portfolio_history))  # æœ€å¤šå–ä¸€å¹´æ•°æ®
            start_value = self.portfolio_history[-recent_window]
            end_value = self.portfolio_history[-1]
            
            if start_value <= 0:
                return 0.0
            
            # è®¡ç®—å¹´åŒ–æ”¶ç›Šç‡
            total_return = (end_value - start_value) / start_value
            periods_per_year = 252 * 24 * 4  # å‡è®¾15åˆ†é’Ÿçº§åˆ«æ•°æ®
            annual_return = total_return * (periods_per_year / recent_window)
            
            # è®¡ç®—ç›¸å¯¹äºç›®æ ‡çš„å¥–åŠ±
            target_return = self.config.annual_return_target
            if annual_return > target_return:
                bonus = (annual_return - target_return) * self.config.annual_return_bonus_scale
                return min(bonus, 2.0)  # é™åˆ¶æœ€å¤§å¥–åŠ±
            else:
                # ä½äºç›®æ ‡æ—¶è½»å¾®æƒ©ç½š
                penalty = (annual_return - target_return) * 0.5
                return max(penalty, -1.0)  # é™åˆ¶æœ€å¤§æƒ©ç½š
        
        except Exception as e:
            self.logger.error(f"å¹´åŒ–æ”¶ç›Šå¥–åŠ±è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _adaptive_parameter_adjustment(self):
        """è‡ªé€‚åº”å‚æ•°è°ƒæ•´"""
        try:
            if self.total_trades < 20:  # éœ€è¦è¶³å¤Ÿçš„äº¤æ˜“æ ·æœ¬
                return
            
            # è®¡ç®—å½“å‰è¡¨ç°æŒ‡æ ‡
            current_winrate = self.winning_trades / self.total_trades if self.total_trades > 0 else 0.5
            current_drawdown = self._calculate_current_drawdown()
            
            # æ ¹æ®è¡¨ç°è°ƒæ•´Î± (èƒœç‡å¥–åŠ±)
            winrate_gap = current_winrate - self.config.winrate_target
            if winrate_gap < -0.1:  # èƒœç‡å¤ªä½ï¼Œå¢åŠ èƒœç‡å¥–åŠ±
                self.current_alpha = min(
                    self.current_alpha * (1 + self.config.alpha_adjustment_rate),
                    self.config.alpha_large_win * 1.5
                )
                self.logger.info(f"ğŸ“ˆ èƒœç‡è¿‡ä½({current_winrate:.2%})ï¼Œå¢åŠ Î±å¥–åŠ±: {self.current_alpha:.2f}")
            elif winrate_gap > 0.1:  # èƒœç‡å¤ªé«˜ï¼Œå¯ä»¥é™ä½å¥–åŠ±
                self.current_alpha = max(
                    self.current_alpha * (1 - self.config.alpha_adjustment_rate),
                    self.config.alpha_small_win * 0.5
                )
            
            # æ ¹æ®è¡¨ç°è°ƒæ•´Î² (äºæŸæƒ©ç½š)
            if current_winrate < self.config.winrate_target:
                # èƒœç‡ä½æ—¶ï¼Œå¢åŠ äºæŸæƒ©ç½š
                self.current_beta = min(
                    self.current_beta * (1 + self.config.beta_adjustment_rate),
                    self.config.beta_large_loss * 1.5
                )
            else:
                # èƒœç‡é«˜æ—¶ï¼Œå¯ä»¥å‡å°‘äºæŸæƒ©ç½š
                self.current_beta = max(
                    self.current_beta * (1 - self.config.beta_adjustment_rate),
                    self.config.beta_small_loss * 0.5
                )
            
            # æ ¹æ®å›æ’¤è°ƒæ•´Î³ (å›æ’¤æƒ©ç½š)
            if current_drawdown > self.config.max_drawdown_target:
                # å›æ’¤è¿‡å¤§ï¼Œå¢åŠ å›æ’¤æƒ©ç½š
                self.current_gamma = min(
                    self.current_gamma * (1 + self.config.gamma_adjustment_rate),
                    self.config.gamma_drawdown * 2.0
                )
                self.logger.info(f"ğŸ“‰ å›æ’¤è¿‡å¤§({current_drawdown:.2%})ï¼Œå¢åŠ Î³æƒ©ç½š: {self.current_gamma:.2f}")
            elif current_drawdown < self.config.max_drawdown_target * 0.5:
                # å›æ’¤å¾ˆå°ï¼Œå¯ä»¥é€‚åº¦é™ä½å›æ’¤æƒ©ç½š
                self.current_gamma = max(
                    self.current_gamma * (1 - self.config.gamma_adjustment_rate),
                    self.config.gamma_drawdown * 0.5
                )
        
        except Exception as e:
            self.logger.error(f"è‡ªé€‚åº”å‚æ•°è°ƒæ•´å¤±è´¥: {e}")
    
    def get_performance_summary(self) -> Dict:
        """è·å–è¡¨ç°æ‘˜è¦"""
        if self.total_trades == 0:
            return {
                'total_trades': 0,
                'win_rate': 0.0,
                'current_drawdown': 0.0,
                'annual_return': 0.0,
                'sharpe_ratio': 0.0
            }
        
        # è®¡ç®—è¡¨ç°æŒ‡æ ‡
        win_rate = self.winning_trades / self.total_trades
        current_drawdown = self._calculate_current_drawdown()
        
        # è®¡ç®—å¹´åŒ–æ”¶ç›Š
        if len(self.portfolio_history) >= 2:
            total_return = (self.portfolio_history[-1] - self.portfolio_history[0]) / self.portfolio_history[0]
            annual_return = total_return * (252 * 24 * 4 / len(self.portfolio_history))
        else:
            annual_return = 0.0
        
        # è®¡ç®—å¤æ™®æ¯”ç‡
        if len(self.return_history) > 1:
            returns = np.array(self.return_history)
            sharpe_ratio = np.mean(returns) / (np.std(returns) + 1e-8) * np.sqrt(252 * 24 * 4)
        else:
            sharpe_ratio = 0.0
        
        return {
            'total_trades': self.total_trades,
            'winning_trades': self.winning_trades,
            'losing_trades': self.losing_trades,
            'win_rate': win_rate,
            'current_drawdown': current_drawdown,
            'max_drawdown': max(self._calculate_historical_max_drawdown(), current_drawdown),
            'annual_return': annual_return,
            'sharpe_ratio': sharpe_ratio,
            'consecutive_wins': self.consecutive_wins,
            'consecutive_losses': self.consecutive_losses,
            'current_parameters': {
                'alpha': self.current_alpha,
                'beta': self.current_beta,
                'gamma': self.current_gamma
            }
        }
    
    def _calculate_historical_max_drawdown(self) -> float:
        """è®¡ç®—å†å²æœ€å¤§å›æ’¤"""
        if len(self.portfolio_history) < 2:
            return 0.0
        
        values = np.array(self.portfolio_history)
        cumulative_max = np.maximum.accumulate(values)
        drawdowns = (cumulative_max - values) / cumulative_max
        
        return np.max(drawdowns)
    
    def reset_for_new_episode(self):
        """é‡ç½®ä¸ºæ–°çš„è®­ç»ƒå›åˆ"""
        self.portfolio_history.clear()
        self.return_history.clear()
        self.reward_history.clear()
        self.peak_portfolio_value = 0.0
        self.total_trades = 0
        self.winning_trades = 0
        self.losing_trades = 0
        self.consecutive_wins = 0
        self.consecutive_losses = 0
        
        # é‡ç½®åŠ¨æ€å‚æ•°
        self.current_alpha = self.config.alpha_medium_win
        self.current_beta = self.config.beta_medium_loss
        self.current_gamma = self.config.gamma_drawdown
        
        self.logger.debug("ğŸ”„ å¥–åŠ±å‡½æ•°é‡ç½®å®Œæˆ")

def create_reward_config(objective: RewardObjective = RewardObjective.BALANCED) -> BalancedRewardConfig:
    """
    æ ¹æ®ç›®æ ‡ç±»å‹åˆ›å»ºå¥–åŠ±é…ç½®
    
    Args:
        objective: å¥–åŠ±ç›®æ ‡ç±»å‹
        
    Returns:
        é…ç½®å¥½çš„å¥–åŠ±å‡½æ•°é…ç½®
    """
    base_config = BalancedRewardConfig()
    
    if objective == RewardObjective.HIGH_WINRATE:
        # é«˜èƒœç‡å¯¼å‘ï¼šå¢å¼ºèƒœç‡å¥–åŠ±ï¼ŒåŠ é‡äºæŸæƒ©ç½š
        base_config.alpha_small_win = 0.8
        base_config.alpha_medium_win = 1.5
        base_config.alpha_large_win = 2.5
        base_config.beta_small_loss = 1.2
        base_config.beta_medium_loss = 2.5
        base_config.beta_large_loss = 4.0
        base_config.winrate_target = 0.70
        
    elif objective == RewardObjective.HIGH_RETURN:
        # é«˜æ”¶ç›Šå¯¼å‘ï¼šå¢å¼ºæ”¶ç›Šå¥–åŠ±ï¼Œé™ä½äºæŸæƒ©ç½š
        base_config.return_scale_factor = 150.0
        base_config.alpha_large_win = 3.0
        base_config.beta_small_loss = 0.5
        base_config.beta_medium_loss = 1.2
        base_config.beta_large_loss = 2.5
        base_config.annual_return_target = 0.50
        base_config.annual_return_bonus_scale = 2.0
        
    elif objective == RewardObjective.LOW_RISK:
        # ä½é£é™©å¯¼å‘ï¼šå¼ºåŒ–å›æ’¤æ§åˆ¶
        base_config.gamma_drawdown = 3.0
        base_config.max_drawdown_target = 0.10
        base_config.max_drawdown_penalty = 8.0
        base_config.return_scale_factor = 80.0
        base_config.alpha_large_win = 1.5
        
    elif objective == RewardObjective.ADAPTIVE:
        # è‡ªé€‚åº”ï¼šå¯ç”¨æ‰€æœ‰è‡ªé€‚åº”åŠŸèƒ½
        base_config.enable_adaptive = True
        base_config.alpha_adjustment_rate = 0.15
        base_config.beta_adjustment_rate = 0.15
        base_config.gamma_adjustment_rate = 0.10
        
    return base_config

def main():
    """æµ‹è¯•å¹³è¡¡å¥–åŠ±å‡½æ•°"""
    # åˆ›å»ºä¸åŒç›®æ ‡çš„é…ç½®
    configs = {
        'balanced': create_reward_config(RewardObjective.BALANCED),
        'high_winrate': create_reward_config(RewardObjective.HIGH_WINRATE),
        'high_return': create_reward_config(RewardObjective.HIGH_RETURN),
        'low_risk': create_reward_config(RewardObjective.LOW_RISK),
    }
    
    print("="*80)
    print("å¹³è¡¡å¥–åŠ±å‡½æ•°é…ç½®å¯¹æ¯”")
    print("="*80)
    
    for name, config in configs.items():
        print(f"\n{name.upper()} é…ç½®:")
        print(f"  Î± (èƒœç‡å¥–åŠ±): å°={config.alpha_small_win}, ä¸­={config.alpha_medium_win}, å¤§={config.alpha_large_win}")
        print(f"  Î² (äºæŸæƒ©ç½š): å°={config.beta_small_loss}, ä¸­={config.beta_medium_loss}, å¤§={config.beta_large_loss}")
        print(f"  Î³ (å›æ’¤æƒ©ç½š): {config.gamma_drawdown}")
        print(f"  æ”¶ç›Šç¼©æ”¾: {config.return_scale_factor}")
        print(f"  ç›®æ ‡èƒœç‡: {config.winrate_target:.1%}")
        print(f"  ç›®æ ‡å¹´åŒ–: {config.annual_return_target:.1%}")
        print(f"  ç›®æ ‡å›æ’¤: {config.max_drawdown_target:.1%}")
    
    # æ¨¡æ‹Ÿæµ‹è¯•
    print("\n" + "="*80)
    print("æ¨¡æ‹Ÿæµ‹è¯•")
    print("="*80)
    
    reward_func = BalancedRewardFunction(configs['balanced'])
    
    # æ¨¡æ‹Ÿä¸€ç³»åˆ—äº¤æ˜“
    portfolio_values = [10000, 10200, 10150, 10300, 10250, 10400, 10350, 10500]
    trade_results = [None, 0.02, -0.005, 0.015, -0.01, 0.018, -0.008, 0.025]
    
    total_rewards = []
    
    for i in range(1, len(portfolio_values)):
        trade_completed = trade_results[i] is not None
        reward, breakdown = reward_func.calculate_reward(
            current_portfolio_value=portfolio_values[i],
            previous_portfolio_value=portfolio_values[i-1],
            action=3 if trade_completed else 0,
            trade_completed=trade_completed,
            trade_pnl_pct=trade_results[i]
        )
        total_rewards.append(reward)
        
        if trade_completed:
            print(f"äº¤æ˜“ {i}: PnL={trade_results[i]*100:.1f}%, å¥–åŠ±={reward:.3f}")
            print(f"  åˆ†è§£: R_t={breakdown['R_t']:.3f}, Î±={breakdown['alpha_term']:.3f}, "
                  f"Î²={breakdown['beta_term']:.3f}, Î³={breakdown['gamma_term']:.3f}")
    
    # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
    summary = reward_func.get_performance_summary()
    print(f"\næœ€ç»ˆç»Ÿè®¡:")
    print(f"  æ€»äº¤æ˜“: {summary['total_trades']}")
    print(f"  èƒœç‡: {summary['win_rate']:.1%}")
    print(f"  å½“å‰å›æ’¤: {summary['current_drawdown']:.1%}")
    print(f"  æ€»å¥–åŠ±: {sum(total_rewards):.3f}")

if __name__ == "__main__":
    main() 