"""
æ•°æ®åˆ’åˆ†æ¨¡å—
è´Ÿè´£å°†æ—¶åºæ•°æ®æŒ‰ç…§æ—¶é—´é¡ºåºæ­£ç¡®åˆ’åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†
ç¡®ä¿ä¸å‘ç”Ÿæ•°æ®æ³„éœ²é—®é¢˜
"""
import pandas as pd
import numpy as np
from typing import Tuple, Dict, Optional
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

from utils.config import get_config
from utils.logger import get_logger

class TimeSeriesDataSplitter:
    """æ—¶åºæ•°æ®åˆ’åˆ†å™¨ - é˜²æ­¢æ•°æ®æ³„éœ²"""
    
    def __init__(self):
        self.config = get_config()
        self.logger = get_logger('DataSplitter', 'data_splitter.log')
        
        # åˆ’åˆ†é…ç½®
        self.train_ratio = self.config.get('TRAIN_TEST_SPLIT_RATIO', 0.8)
        self.validation_ratio = self.config.get('VALIDATION_SPLIT_RATIO', 0.1)
        self.use_time_split = self.config.get('USE_TIME_SPLIT', True)
        self.prevent_leakage = self.config.get('PREVENT_DATA_LEAKAGE', True)
        
        # è®¡ç®—æµ‹è¯•é›†æ¯”ä¾‹
        self.test_ratio = 1.0 - self.train_ratio - self.validation_ratio
        
        self.logger.info(f"æ•°æ®åˆ’åˆ†é…ç½®: è®­ç»ƒé›†={self.train_ratio*100:.1f}%, "
                        f"éªŒè¯é›†={self.validation_ratio*100:.1f}%, "
                        f"æµ‹è¯•é›†={self.test_ratio*100:.1f}%")
    
    def split_data(self, df: pd.DataFrame, shuffle: bool = False) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """
        å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†
        
        Args:
            df: è¾“å…¥æ•°æ®DataFrameï¼Œå¿…é¡»åŒ…å«æ—¶é—´ç´¢å¼•
            shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®ï¼ˆå¯¹æ—¶åºæ•°æ®ä¸æ¨èï¼‰
            
        Returns:
            (train_df, val_df, test_df): è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†
        """
        if df.empty:
            self.logger.error("è¾“å…¥æ•°æ®ä¸ºç©º")
            return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()
        
        # ç¡®ä¿æ•°æ®æŒ‰æ—¶é—´æ’åº
        if not df.index.is_monotonic_increasing:
            self.logger.warning("æ•°æ®æœªæŒ‰æ—¶é—´æ’åºï¼Œæ­£åœ¨é‡æ–°æ’åº...")
            df = df.sort_index()
        
        total_len = len(df)
        
        if self.use_time_split:
            # æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†ï¼ˆæ¨èæ–¹å¼ï¼‰
            train_df, val_df, test_df = self._time_based_split(df)
        else:
            # éšæœºåˆ’åˆ†ï¼ˆä¸æ¨èç”¨äºæ—¶åºæ•°æ®ï¼‰
            if not shuffle:
                self.logger.warning("éæ—¶é—´åˆ’åˆ†æ¨¡å¼å»ºè®®å¯ç”¨shuffleå‚æ•°")
            train_df, val_df, test_df = self._random_split(df, shuffle)
        
        # éªŒè¯åˆ’åˆ†ç»“æœ
        self._validate_split(train_df, val_df, test_df, df)
        
        # è®°å½•åˆ’åˆ†ä¿¡æ¯
        self._log_split_info(train_df, val_df, test_df)
        
        return train_df, val_df, test_df
    
    def _time_based_split(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†æ•°æ®"""
        total_len = len(df)
        
        # è®¡ç®—åˆ†å‰²ç‚¹
        train_end_idx = int(total_len * self.train_ratio)
        val_end_idx = int(total_len * (self.train_ratio + self.validation_ratio))
        
        # æ—¶é—´é¡ºåºåˆ’åˆ†ï¼šè®­ç»ƒé›† -> éªŒè¯é›† -> æµ‹è¯•é›†
        train_df = df.iloc[:train_end_idx].copy()
        val_df = df.iloc[train_end_idx:val_end_idx].copy()
        test_df = df.iloc[val_end_idx:].copy()
        
        # æ·»åŠ æ•°æ®é›†æ ‡è¯†
        train_df.loc[:, 'dataset_type'] = 'train'
        val_df.loc[:, 'dataset_type'] = 'validation'
        test_df.loc[:, 'dataset_type'] = 'test'
        
        return train_df, val_df, test_df
    
    def _random_split(self, df: pd.DataFrame, shuffle: bool) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """éšæœºåˆ’åˆ†æ•°æ®ï¼ˆä¸æ¨èç”¨äºæ—¶åºæ•°æ®ï¼‰"""
        self.logger.warning("ä½¿ç”¨éšæœºåˆ’åˆ†å¯èƒ½å¯¼è‡´æ•°æ®æ³„éœ²ï¼Œå¼ºçƒˆå»ºè®®ä½¿ç”¨æ—¶é—´åˆ’åˆ†")
        
        total_len = len(df)
        indices = np.arange(total_len)
        
        if shuffle:
            np.random.seed(42)  # å›ºå®šéšæœºç§å­ä¿è¯å¯é‡å¤æ€§
            np.random.shuffle(indices)
        
        # è®¡ç®—åˆ†å‰²ç‚¹
        train_end_idx = int(total_len * self.train_ratio)
        val_end_idx = int(total_len * (self.train_ratio + self.validation_ratio))
        
        # è·å–ç´¢å¼•
        train_indices = indices[:train_end_idx]
        val_indices = indices[train_end_idx:val_end_idx]
        test_indices = indices[val_end_idx:]
        
        # åˆ’åˆ†æ•°æ®
        train_df = df.iloc[train_indices].copy()
        val_df = df.iloc[val_indices].copy()
        test_df = df.iloc[test_indices].copy()
        
        # é‡æ–°æŒ‰æ—¶é—´æ’åº
        train_df = train_df.sort_index()
        val_df = val_df.sort_index()
        test_df = test_df.sort_index()
        
        # æ·»åŠ æ•°æ®é›†æ ‡è¯†
        train_df.loc[:, 'dataset_type'] = 'train'
        val_df.loc[:, 'dataset_type'] = 'validation'
        test_df.loc[:, 'dataset_type'] = 'test'
        
        return train_df, val_df, test_df
    
    def _validate_split(self, train_df: pd.DataFrame, val_df: pd.DataFrame, 
                       test_df: pd.DataFrame, original_df: pd.DataFrame):
        """éªŒè¯æ•°æ®åˆ’åˆ†çš„æ­£ç¡®æ€§"""
        total_original = len(original_df)
        total_split = len(train_df) + len(val_df) + len(test_df)
        
        # æ£€æŸ¥æ•°æ®æ€»é‡
        if total_split != total_original:
            self.logger.error(f"æ•°æ®åˆ’åˆ†é”™è¯¯ï¼šåŸå§‹æ•°æ®{total_original}è¡Œï¼Œåˆ’åˆ†å{total_split}è¡Œ")
            raise ValueError("æ•°æ®åˆ’åˆ†è¿‡ç¨‹ä¸­ä¸¢å¤±äº†æ•°æ®")
        
        # æ£€æŸ¥æ—¶é—´åºåˆ—è¿ç»­æ€§ï¼ˆä»…å¯¹æ—¶é—´åˆ’åˆ†ï¼‰
        if self.use_time_split and self.prevent_leakage:
            self._check_time_leakage(train_df, val_df, test_df)
        
        # æ£€æŸ¥æ•°æ®åˆ†å¸ƒ
        self._check_data_distribution(train_df, val_df, test_df, original_df)
    
    def _check_time_leakage(self, train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame):
        """æ£€æŸ¥æ—¶é—´æ³„éœ²é—®é¢˜"""
        try:
            # è·å–å„æ•°æ®é›†çš„æ—¶é—´èŒƒå›´
            train_max_time = train_df.index.max()
            val_min_time = val_df.index.min()
            val_max_time = val_df.index.max()
            test_min_time = test_df.index.min()
            
            # æ£€æŸ¥è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¹‹é—´çš„æ—¶é—´é¡ºåº
            if train_max_time >= val_min_time:
                self.logger.warning(f"æ½œåœ¨æ—¶é—´æ³„éœ²ï¼šè®­ç»ƒé›†æœ€æ™šæ—¶é—´({train_max_time}) >= éªŒè¯é›†æœ€æ—©æ—¶é—´({val_min_time})")
            
            # æ£€æŸ¥éªŒè¯é›†å’Œæµ‹è¯•é›†ä¹‹é—´çš„æ—¶é—´é¡ºåº
            if val_max_time >= test_min_time:
                self.logger.warning(f"æ½œåœ¨æ—¶é—´æ³„éœ²ï¼šéªŒè¯é›†æœ€æ™šæ—¶é—´({val_max_time}) >= æµ‹è¯•é›†æœ€æ—©æ—¶é—´({test_min_time})")
            
            # å¦‚æœæ²¡æœ‰é—®é¢˜ï¼Œè®°å½•æˆåŠŸ
            if train_max_time < val_min_time < val_max_time < test_min_time:
                self.logger.info("âœ… æ—¶é—´åºåˆ—åˆ’åˆ†æ­£ç¡®ï¼Œæ— æ•°æ®æ³„éœ²é£é™©")
            
        except Exception as e:
            self.logger.error(f"æ—¶é—´æ³„éœ²æ£€æŸ¥å¤±è´¥: {e}")
    
    def _check_data_distribution(self, train_df: pd.DataFrame, val_df: pd.DataFrame, 
                               test_df: pd.DataFrame, original_df: pd.DataFrame):
        """æ£€æŸ¥æ•°æ®åˆ†å¸ƒ"""
        try:
            # æ£€æŸ¥ä»·æ ¼åˆ†å¸ƒ
            if 'close' in original_df.columns:
                original_price_stats = original_df['close'].describe()
                train_price_stats = train_df['close'].describe()
                val_price_stats = val_df['close'].describe()
                test_price_stats = test_df['close'].describe()
                
                self.logger.info("ä»·æ ¼åˆ†å¸ƒç»Ÿè®¡:")
                self.logger.info(f"  åŸå§‹æ•°æ®: å‡å€¼={original_price_stats['mean']:.2f}, æ ‡å‡†å·®={original_price_stats['std']:.2f}")
                self.logger.info(f"  è®­ç»ƒé›†: å‡å€¼={train_price_stats['mean']:.2f}, æ ‡å‡†å·®={train_price_stats['std']:.2f}")
                self.logger.info(f"  éªŒè¯é›†: å‡å€¼={val_price_stats['mean']:.2f}, æ ‡å‡†å·®={val_price_stats['std']:.2f}")
                self.logger.info(f"  æµ‹è¯•é›†: å‡å€¼={test_price_stats['mean']:.2f}, æ ‡å‡†å·®={test_price_stats['std']:.2f}")
                
        except Exception as e:
            self.logger.warning(f"æ•°æ®åˆ†å¸ƒæ£€æŸ¥å¤±è´¥: {e}")
    
    def _log_split_info(self, train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame):
        """è®°å½•åˆ’åˆ†ä¿¡æ¯"""
        self.logger.info("=" * 60)
        self.logger.info("æ•°æ®åˆ’åˆ†å®Œæˆ")
        self.logger.info(f"è®­ç»ƒé›†: {len(train_df):,} è¡Œ ({len(train_df)/(len(train_df)+len(val_df)+len(test_df))*100:.1f}%)")
        self.logger.info(f"éªŒè¯é›†: {len(val_df):,} è¡Œ ({len(val_df)/(len(train_df)+len(val_df)+len(test_df))*100:.1f}%)")
        self.logger.info(f"æµ‹è¯•é›†: {len(test_df):,} è¡Œ ({len(test_df)/(len(train_df)+len(val_df)+len(test_df))*100:.1f}%)")
        
        if not train_df.empty and not test_df.empty:
            self.logger.info(f"è®­ç»ƒé›†æ—¶é—´èŒƒå›´: {train_df.index.min()} åˆ° {train_df.index.max()}")
            if not val_df.empty:
                self.logger.info(f"éªŒè¯é›†æ—¶é—´èŒƒå›´: {val_df.index.min()} åˆ° {val_df.index.max()}")
            self.logger.info(f"æµ‹è¯•é›†æ—¶é—´èŒƒå›´: {test_df.index.min()} åˆ° {test_df.index.max()}")
        
        self.logger.info("=" * 60)
    
    def create_train_test_only(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        ä»…åˆ›å»ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆä¸è¦éªŒè¯é›†ï¼‰
        
        Args:
            df: è¾“å…¥æ•°æ®
            
        Returns:
            (train_df, test_df): è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        """
        # ä¸´æ—¶è°ƒæ•´æ¯”ä¾‹
        original_train_ratio = self.train_ratio
        original_val_ratio = self.validation_ratio
        
        # å°†éªŒè¯é›†æ¯”ä¾‹åŠ åˆ°è®­ç»ƒé›†
        self.train_ratio = original_train_ratio + original_val_ratio
        self.validation_ratio = 0.0
        
        try:
            train_df, _, test_df = self.split_data(df)
            return train_df, test_df
        finally:
            # æ¢å¤åŸå§‹æ¯”ä¾‹
            self.train_ratio = original_train_ratio
            self.validation_ratio = original_val_ratio
    
    def get_split_summary(self, train_df: pd.DataFrame, val_df: pd.DataFrame, 
                         test_df: pd.DataFrame) -> Dict:
        """è·å–æ•°æ®åˆ’åˆ†æ‘˜è¦"""
        total_samples = len(train_df) + len(val_df) + len(test_df)
        
        summary = {
            'total_samples': total_samples,
            'train_samples': len(train_df),
            'validation_samples': len(val_df),
            'test_samples': len(test_df),
            'train_percentage': len(train_df) / total_samples * 100,
            'validation_percentage': len(val_df) / total_samples * 100,
            'test_percentage': len(test_df) / total_samples * 100,
            'split_method': 'time_based' if self.use_time_split else 'random',
            'leakage_prevention': self.prevent_leakage
        }
        
        if not train_df.empty and not test_df.empty:
            summary.update({
                'train_time_range': (train_df.index.min(), train_df.index.max()),
                'test_time_range': (test_df.index.min(), test_df.index.max())
            })
            
            if not val_df.empty:
                summary['validation_time_range'] = (val_df.index.min(), val_df.index.max())
        
        return summary

def main():
    """æµ‹è¯•æ•°æ®åˆ’åˆ†åŠŸèƒ½"""
    from data_collector import DataCollector
    
    print("ğŸ”„ æµ‹è¯•æ•°æ®åˆ’åˆ†åŠŸèƒ½")
    
    # åŠ è½½æ•°æ®
    collector = DataCollector()
    df = collector.load_data()
    
    if df.empty:
        print("âŒ æ²¡æœ‰å¯ç”¨æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®æ”¶é›†")
        return
    
    # åˆ›å»ºæ•°æ®åˆ’åˆ†å™¨
    splitter = TimeSeriesDataSplitter()
    
    # åˆ’åˆ†æ•°æ®
    train_df, val_df, test_df = splitter.split_data(df)
    
    # è·å–æ‘˜è¦
    summary = splitter.get_split_summary(train_df, val_df, test_df)
    
    print("ğŸ“Š æ•°æ®åˆ’åˆ†æ‘˜è¦:")
    for key, value in summary.items():
        print(f"  {key}: {value}")

if __name__ == "__main__":
    main() 