"""
ç‰¹å¾åˆ†æå™¨ - ç‰¹å¾ç›‘æ§å’Œå¯è§†åŒ–å·¥å…·
æä¾›ç‰¹å¾é‡è¦æ€§åˆ†æã€ç›¸å…³æ€§çƒ­å›¾ã€ç‰¹å¾åˆ†å¸ƒåˆ†æç­‰åŠŸèƒ½
å¸®åŠ©ä¼˜åŒ–31ä¸ªç‰¹å¾ï¼Œæå‡æ¨¡å‹æ€§èƒ½
"""
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional, Any
import warnings
warnings.filterwarnings('ignore')

from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px

from utils.config import get_config
from utils.logger import get_logger

class FeatureAnalyzer:
    """ç‰¹å¾åˆ†æå™¨ - ç›‘æ§å’Œä¼˜åŒ–ç‰¹å¾è´¨é‡"""
    
    def __init__(self):
        self.config = get_config()
        self.logger = get_logger('FeatureAnalyzer', 'feature_analyzer.log')
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # é…è‰²æ–¹æ¡ˆ
        self.colors = {
            'primary': '#1f77b4',
            'secondary': '#ff7f0e', 
            'success': '#2ca02c',
            'warning': '#d62728',
            'info': '#9467bd',
            'light': '#17becf'
        }
    
    def create_comprehensive_feature_report(self, df: pd.DataFrame, 
                                          feature_importance: Dict = None,
                                          save_path: str = None) -> str:
        """
        âœ… åˆ›å»ºå…¨é¢çš„ç‰¹å¾åˆ†ææŠ¥å‘Š
        """
        try:
            self.logger.info("ğŸ“Š å¼€å§‹åˆ›å»ºç‰¹å¾åˆ†ææŠ¥å‘Š...")
            
            # åˆ›å»ºå­å›¾
            fig = make_subplots(
                rows=4, cols=3,
                subplot_titles=[
                    'ç‰¹å¾é‡è¦æ€§æ’åº', 'ç‰¹å¾ç›¸å…³æ€§çƒ­å›¾', 'ç‰¹å¾åˆ†å¸ƒæ¦‚è§ˆ',
                    'ç‰¹å¾ç¼ºå¤±å€¼åˆ†æ', 'PCAæ–¹å·®è§£é‡Š', 'ç‰¹å¾èšç±»åˆ†æ',
                    'ç‰¹å¾ç¨³å®šæ€§åˆ†æ', 'ç‰¹å¾ååº¦åˆ†æ', 'ç‰¹å¾å¼‚å¸¸å€¼æ£€æµ‹',
                    'æ—¶é—´åºåˆ—ç‰¹å¾å˜åŒ–', 'ç‰¹å¾äº’ä¿¡æ¯çŸ©é˜µ', 'ç‰¹å¾é€‰æ‹©å»ºè®®'
                ],
                specs=[
                    [{"type": "bar"}, {"type": "heatmap"}, {"type": "histogram"}],
                    [{"type": "bar"}, {"type": "scatter"}, {"type": "scatter"}],
                    [{"type": "line"}, {"type": "bar"}, {"type": "box"}],
                    [{"type": "line"}, {"type": "heatmap"}, {"type": "table"}]
                ],
                vertical_spacing=0.08,
                horizontal_spacing=0.08
            )
            
            # 1. ç‰¹å¾é‡è¦æ€§åˆ†æ
            self._add_feature_importance_plot(fig, feature_importance, row=1, col=1)
            
            # 2. ç›¸å…³æ€§çƒ­å›¾
            self._add_correlation_heatmap(fig, df, row=1, col=2)
            
            # 3. ç‰¹å¾åˆ†å¸ƒ
            self._add_feature_distribution_plot(fig, df, row=1, col=3)
            
            # 4. ç¼ºå¤±å€¼åˆ†æ
            self._add_missing_values_analysis(fig, df, row=2, col=1)
            
            # 5. PCAåˆ†æ
            self._add_pca_analysis(fig, df, row=2, col=2)
            
            # 6. ç‰¹å¾èšç±»
            self._add_feature_clustering(fig, df, row=2, col=3)
            
            # 7. ç‰¹å¾ç¨³å®šæ€§
            self._add_stability_analysis(fig, df, row=3, col=1)
            
            # 8. ååº¦åˆ†æ
            self._add_skewness_analysis(fig, df, row=3, col=2)
            
            # 9. å¼‚å¸¸å€¼æ£€æµ‹
            self._add_outlier_detection(fig, df, row=3, col=3)
            
            # 10. æ—¶é—´åºåˆ—å˜åŒ–
            self._add_temporal_analysis(fig, df, row=4, col=1)
            
            # 11. äº’ä¿¡æ¯çŸ©é˜µ
            self._add_mutual_information_matrix(fig, df, row=4, col=2)
            
            # 12. ç‰¹å¾é€‰æ‹©å»ºè®®
            self._add_feature_selection_recommendations(fig, df, feature_importance, row=4, col=3)
            
            # æ›´æ–°å¸ƒå±€
            fig.update_layout(
                title={
                    'text': 'SOL/USDT ç‰¹å¾å·¥ç¨‹å…¨é¢åˆ†ææŠ¥å‘Š',
                    'x': 0.5,
                    'font': {'size': 20}
                },
                height=1600,
                showlegend=False,
                template='plotly_white'
            )
            
            # ä¿å­˜æŠ¥å‘Š
            if save_path is None:
                save_path = f"results/feature_analysis_report.html"
            
            fig.write_html(save_path)
            self.logger.info(f"âœ… ç‰¹å¾åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {save_path}")
            
            return save_path
            
        except Exception as e:
            self.logger.exception(f"âŒ åˆ›å»ºç‰¹å¾åˆ†ææŠ¥å‘Šå¤±è´¥: {e}")
            return None
    
    def _add_feature_importance_plot(self, fig, feature_importance: Dict, row: int, col: int):
        """æ·»åŠ ç‰¹å¾é‡è¦æ€§å›¾"""
        try:
            if not feature_importance or 'combined_score' not in feature_importance:
                return
            
            # è·å–top20ç‰¹å¾
            scores = feature_importance['combined_score']
            top_features = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:20]
            
            features, importance = zip(*top_features)
            
            fig.add_trace(
                go.Bar(
                    x=list(importance),
                    y=list(features),
                    orientation='h',
                    name='ç‰¹å¾é‡è¦æ€§',
                    marker_color=self.colors['primary']
                ),
                row=row, col=col
            )
            
            fig.update_xaxes(title_text="é‡è¦æ€§è¯„åˆ†", row=row, col=col)
            
        except Exception as e:
            self.logger.error(f"æ·»åŠ ç‰¹å¾é‡è¦æ€§å›¾å¤±è´¥: {e}")
    
    def _add_correlation_heatmap(self, fig, df: pd.DataFrame, row: int, col: int):
        """æ·»åŠ ç›¸å…³æ€§çƒ­å›¾"""
        try:
            # è·å–æ•°å€¼ç‰¹å¾
            numeric_cols = df.select_dtypes(include=[np.number]).columns[:20]  # é™åˆ¶æ˜¾ç¤ºå‰20ä¸ª
            corr_matrix = df[numeric_cols].corr()
            
            fig.add_trace(
                go.Heatmap(
                    z=corr_matrix.values,
                    x=corr_matrix.columns,
                    y=corr_matrix.columns,
                    colorscale='RdBu',
                    zmid=0,
                    name='ç›¸å…³æ€§'
                ),
                row=row, col=col
            )
            
        except Exception as e:
            self.logger.error(f"æ·»åŠ ç›¸å…³æ€§çƒ­å›¾å¤±è´¥: {e}")
    
    def _add_feature_distribution_plot(self, fig, df: pd.DataFrame, row: int, col: int):
        """æ·»åŠ ç‰¹å¾åˆ†å¸ƒå›¾"""
        try:
            # éšæœºé€‰æ‹©å‡ ä¸ªç‰¹å¾å±•ç¤ºåˆ†å¸ƒ
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            sample_cols = np.random.choice(numeric_cols, min(5, len(numeric_cols)), replace=False)
            
            for i, col in enumerate(sample_cols):
                fig.add_trace(
                    go.Histogram(
                        x=df[col].dropna(),
                        name=f'{col}',
                        opacity=0.7,
                        nbinsx=30
                    ),
                    row=row, col=col
                )
            
            fig.update_xaxes(title_text="ç‰¹å¾å€¼", row=row, col=col)
            fig.update_yaxes(title_text="é¢‘æ¬¡", row=row, col=col)
            
        except Exception as e:
            self.logger.error(f"æ·»åŠ ç‰¹å¾åˆ†å¸ƒå›¾å¤±è´¥: {e}")
    
    def _add_missing_values_analysis(self, fig, df: pd.DataFrame, row: int, col: int):
        """æ·»åŠ ç¼ºå¤±å€¼åˆ†æ"""
        try:
            missing_data = df.isnull().sum()
            missing_data = missing_data[missing_data > 0].sort_values(ascending=False)
            
            if len(missing_data) > 0:
                fig.add_trace(
                    go.Bar(
                        x=missing_data.index,
                        y=missing_data.values,
                        name='ç¼ºå¤±å€¼æ•°é‡',
                        marker_color=self.colors['warning']
                    ),
                    row=row, col=col
                )
            
            fig.update_xaxes(title_text="ç‰¹å¾", row=row, col=col)
            fig.update_yaxes(title_text="ç¼ºå¤±å€¼æ•°é‡", row=row, col=col)
            
        except Exception as e:
            self.logger.error(f"æ·»åŠ ç¼ºå¤±å€¼åˆ†æå¤±è´¥: {e}")
    
    def _add_pca_analysis(self, fig, df: pd.DataFrame, row: int, col: int):
        """æ·»åŠ PCAåˆ†æ"""
        try:
            # è·å–æ•°å€¼ç‰¹å¾
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            X = df[numeric_cols].fillna(0)
            
            # è¿›è¡ŒPCA
            pca = PCA(n_components=min(10, len(numeric_cols)))
            pca.fit(X)
            
            # ç´¯ç§¯æ–¹å·®è§£é‡Šç‡
            cumulative_variance = np.cumsum(pca.explained_variance_ratio_)
            
            fig.add_trace(
                go.Scatter(
                    x=list(range(1, len(cumulative_variance) + 1)),
                    y=cumulative_variance,
                    mode='lines+markers',
                    name='ç´¯ç§¯æ–¹å·®è§£é‡Šç‡',
                    line=dict(color=self.colors['success'])
                ),
                row=row, col=col
            )
            
            fig.update_xaxes(title_text="ä¸»æˆåˆ†æ•°é‡", row=row, col=col)
            fig.update_yaxes(title_text="ç´¯ç§¯æ–¹å·®è§£é‡Šç‡", row=row, col=col)
            
        except Exception as e:
            self.logger.error(f"æ·»åŠ PCAåˆ†æå¤±è´¥: {e}")
    
    def _add_feature_clustering(self, fig, df: pd.DataFrame, row: int, col: int):
        """æ·»åŠ ç‰¹å¾èšç±»åˆ†æ"""
        try:
            # è®¡ç®—ç‰¹å¾é—´çš„ç›¸å…³æ€§
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            corr_matrix = df[numeric_cols].corr().abs()
            
            # ä½¿ç”¨ç›¸å…³æ€§è¿›è¡Œèšç±»
            from scipy.cluster.hierarchy import linkage, dendrogram
            from scipy.spatial.distance import squareform
            
            # è½¬æ¢ä¸ºè·ç¦»çŸ©é˜µ
            distance_matrix = 1 - corr_matrix
            condensed_distances = squareform(distance_matrix)
            
            # è¿›è¡Œå±‚æ¬¡èšç±»
            linkage_matrix = linkage(condensed_distances, method='ward')
            
            # è¿™é‡Œç®€åŒ–æ˜¾ç¤ºï¼Œå®é™…å¯ä»¥ç”¨dendrogram
            # æ”¹ä¸ºæ˜¾ç¤ºèšç±»è´¨é‡è¯„ä¼°
            silhouette_scores = []
            K_range = range(2, min(10, len(numeric_cols)))
            
            for k in K_range:
                kmeans = KMeans(n_clusters=k, random_state=42)
                cluster_labels = kmeans.fit_predict(corr_matrix)
                silhouette_avg = silhouette_score(corr_matrix, cluster_labels)
                silhouette_scores.append(silhouette_avg)
            
            fig.add_trace(
                go.Scatter(
                    x=list(K_range),
                    y=silhouette_scores,
                    mode='lines+markers',
                    name='è½®å»“ç³»æ•°',
                    line=dict(color=self.colors['info'])
                ),
                row=row, col=col
            )
            
            fig.update_xaxes(title_text="èšç±»æ•°é‡", row=row, col=col)
            fig.update_yaxes(title_text="è½®å»“ç³»æ•°", row=row, col=col)
            
        except Exception as e:
            self.logger.error(f"æ·»åŠ ç‰¹å¾èšç±»åˆ†æå¤±è´¥: {e}")
    
    def _add_stability_analysis(self, fig, df: pd.DataFrame, row: int, col: int):
        """æ·»åŠ ç‰¹å¾ç¨³å®šæ€§åˆ†æ"""
        try:
            # è®¡ç®—æ»šåŠ¨æ ‡å‡†å·®æ¥è¯„ä¼°ç¨³å®šæ€§
            numeric_cols = df.select_dtypes(include=[np.number]).columns[:5]  # é€‰æ‹©å‰5ä¸ªç‰¹å¾
            
            for col in numeric_cols:
                rolling_std = df[col].rolling(100).std()
                
                fig.add_trace(
                    go.Scatter(
                        x=df.index,
                        y=rolling_std,
                        mode='lines',
                        name=f'{col}_stability',
                        opacity=0.7
                    ),
                    row=row, col=col
                )
            
            fig.update_xaxes(title_text="æ—¶é—´", row=row, col=col)
            fig.update_yaxes(title_text="æ»šåŠ¨æ ‡å‡†å·®", row=row, col=col)
            
        except Exception as e:
            self.logger.error(f"æ·»åŠ ç¨³å®šæ€§åˆ†æå¤±è´¥: {e}")
    
    def _add_skewness_analysis(self, fig, df: pd.DataFrame, row: int, col: int):
        """æ·»åŠ ååº¦åˆ†æ"""
        try:
            from scipy.stats import skew
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            skewness_values = []
            feature_names = []
            
            for col in numeric_cols:
                skew_val = skew(df[col].dropna())
                skewness_values.append(skew_val)
                feature_names.append(col)
            
            # åªæ˜¾ç¤ºå‰20ä¸ª
            if len(skewness_values) > 20:
                indices = np.argsort(np.abs(skewness_values))[-20:]
                skewness_values = [skewness_values[i] for i in indices]
                feature_names = [feature_names[i] for i in indices]
            
            colors = [self.colors['warning'] if abs(s) > 1 else self.colors['primary'] for s in skewness_values]
            
            fig.add_trace(
                go.Bar(
                    x=feature_names,
                    y=skewness_values,
                    name='ååº¦',
                    marker_color=colors
                ),
                row=row, col=col
            )
            
            fig.update_xaxes(title_text="ç‰¹å¾", row=row, col=col)
            fig.update_yaxes(title_text="ååº¦å€¼", row=row, col=col)
            
        except Exception as e:
            self.logger.error(f"æ·»åŠ ååº¦åˆ†æå¤±è´¥: {e}")
    
    def _add_outlier_detection(self, fig, df: pd.DataFrame, row: int, col: int):
        """æ·»åŠ å¼‚å¸¸å€¼æ£€æµ‹"""
        try:
            # é€‰æ‹©å‡ ä¸ªç‰¹å¾è¿›è¡Œå¼‚å¸¸å€¼æ£€æµ‹
            numeric_cols = df.select_dtypes(include=[np.number]).columns[:5]
            
            outlier_counts = []
            feature_names = []
            
            for col in numeric_cols:
                data = df[col].dropna()
                Q1 = data.quantile(0.25)
                Q3 = data.quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                outliers = data[(data < lower_bound) | (data > upper_bound)]
                outlier_counts.append(len(outliers))
                feature_names.append(col)
            
            fig.add_trace(
                go.Bar(
                    x=feature_names,
                    y=outlier_counts,
                    name='å¼‚å¸¸å€¼æ•°é‡',
                    marker_color=self.colors['warning']
                ),
                row=row, col=col
            )
            
            fig.update_xaxes(title_text="ç‰¹å¾", row=row, col=col)
            fig.update_yaxes(title_text="å¼‚å¸¸å€¼æ•°é‡", row=row, col=col)
            
        except Exception as e:
            self.logger.error(f"æ·»åŠ å¼‚å¸¸å€¼æ£€æµ‹å¤±è´¥: {e}")
    
    def _add_temporal_analysis(self, fig, df: pd.DataFrame, row: int, col: int):
        """æ·»åŠ æ—¶é—´åºåˆ—ç‰¹å¾å˜åŒ–åˆ†æ"""
        try:
            # è®¡ç®—ç‰¹å¾çš„æ—¶é—´ç¨³å®šæ€§
            numeric_cols = df.select_dtypes(include=[np.number]).columns[:3]  # é€‰æ‹©å‰3ä¸ªç‰¹å¾
            
            for col in numeric_cols:
                # è®¡ç®—æ»šåŠ¨å‡å€¼
                rolling_mean = df[col].rolling(100).mean()
                
                fig.add_trace(
                    go.Scatter(
                        x=df.index,
                        y=rolling_mean,
                        mode='lines',
                        name=f'{col}_è¶‹åŠ¿',
                        opacity=0.8
                    ),
                    row=row, col=col
                )
            
            fig.update_xaxes(title_text="æ—¶é—´", row=row, col=col)
            fig.update_yaxes(title_text="æ»šåŠ¨å‡å€¼", row=row, col=col)
            
        except Exception as e:
            self.logger.error(f"æ·»åŠ æ—¶é—´åºåˆ—åˆ†æå¤±è´¥: {e}")
    
    def _add_mutual_information_matrix(self, fig, df: pd.DataFrame, row: int, col: int):
        """æ·»åŠ äº’ä¿¡æ¯çŸ©é˜µ"""
        try:
            from sklearn.feature_selection import mutual_info_regression
            
            # é€‰æ‹©å‰10ä¸ªæ•°å€¼ç‰¹å¾
            numeric_cols = df.select_dtypes(include=[np.number]).columns[:10]
            
            # è®¡ç®—äº’ä¿¡æ¯çŸ©é˜µ
            mi_matrix = np.zeros((len(numeric_cols), len(numeric_cols)))
            
            for i, col1 in enumerate(numeric_cols):
                for j, col2 in enumerate(numeric_cols):
                    if i == j:
                        mi_matrix[i, j] = 1.0  # è‡ªå·±ä¸è‡ªå·±çš„äº’ä¿¡æ¯è®¾ä¸º1
                    elif i < j:
                        try:
                            X = df[col1].fillna(0).values.reshape(-1, 1)
                            y = df[col2].fillna(0).values
                            mi_score = mutual_info_regression(X, y, random_state=42)[0]
                            mi_matrix[i, j] = mi_score
                            mi_matrix[j, i] = mi_score  # å¯¹ç§°çŸ©é˜µ
                        except:
                            mi_matrix[i, j] = 0
                            mi_matrix[j, i] = 0
            
            fig.add_trace(
                go.Heatmap(
                    z=mi_matrix,
                    x=numeric_cols,
                    y=numeric_cols,
                    colorscale='Viridis',
                    name='äº’ä¿¡æ¯'
                ),
                row=row, col=col
            )
            
        except Exception as e:
            self.logger.error(f"æ·»åŠ äº’ä¿¡æ¯çŸ©é˜µå¤±è´¥: {e}")
    
    def _add_feature_selection_recommendations(self, fig, df: pd.DataFrame, 
                                             feature_importance: Dict, row: int, col: int):
        """æ·»åŠ ç‰¹å¾é€‰æ‹©å»ºè®®"""
        try:
            # åˆ›å»ºå»ºè®®è¡¨æ ¼
            recommendations = []
            
            # åŸºäºé‡è¦æ€§çš„å»ºè®®
            if feature_importance and 'combined_score' in feature_importance:
                scores = feature_importance['combined_score']
                low_importance = [k for k, v in scores.items() if v < 0.01]
                recommendations.extend([f"ç§»é™¤ä½é‡è¦æ€§ç‰¹å¾: {', '.join(low_importance[:5])}" if low_importance else "æ— éœ€ç§»é™¤ä½é‡è¦æ€§ç‰¹å¾"])
            
            # åŸºäºç›¸å…³æ€§çš„å»ºè®®
            corr_matrix = df.select_dtypes(include=[np.number]).corr()
            high_corr_pairs = []
            for i in range(len(corr_matrix.columns)):
                for j in range(i+1, len(corr_matrix.columns)):
                    if abs(corr_matrix.iloc[i, j]) > 0.9:
                        high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j]))
            
            if high_corr_pairs:
                recommendations.append(f"å‘ç°{len(high_corr_pairs)}å¯¹é«˜ç›¸å…³ç‰¹å¾ï¼Œå»ºè®®ç§»é™¤")
            else:
                recommendations.append("ç‰¹å¾ç›¸å…³æ€§è‰¯å¥½ï¼Œæ— éœ€ç§»é™¤")
            
            # åŸºäºç¼ºå¤±å€¼çš„å»ºè®®
            missing_features = df.isnull().sum()
            high_missing = missing_features[missing_features > len(df) * 0.3]
            if len(high_missing) > 0:
                recommendations.append(f"ç§»é™¤é«˜ç¼ºå¤±ç‡ç‰¹å¾: {', '.join(high_missing.index[:3])}")
            else:
                recommendations.append("ç¼ºå¤±å€¼æ§åˆ¶è‰¯å¥½")
            
            # æ€»ä½“å»ºè®®
            total_features = len(df.select_dtypes(include=[np.number]).columns)
            if total_features > 50:
                recommendations.append("ç‰¹å¾æ•°é‡è¿‡å¤šï¼Œå»ºè®®é™ç»´è‡³30-40ä¸ª")
            elif total_features < 20:
                recommendations.append("ç‰¹å¾æ•°é‡åå°‘ï¼Œå¯è€ƒè™‘å¢åŠ æŠ€æœ¯æŒ‡æ ‡")
            else:
                recommendations.append("ç‰¹å¾æ•°é‡é€‚ä¸­")
            
            # ç®€åŒ–æ˜¾ç¤ºä¸ºæ–‡æœ¬
            recommendation_text = "<br>".join([f"{i+1}. {rec}" for i, rec in enumerate(recommendations)])
            
            fig.add_annotation(
                text=recommendation_text,
                xref=f"x{col}" if row > 1 or col > 1 else "x",
                yref=f"y{((row-1)*3 + col)}" if row > 1 or col > 1 else "y",
                x=0.5, y=0.5,
                showarrow=False,
                font=dict(size=12),
                align="left"
            )
            
        except Exception as e:
            self.logger.error(f"æ·»åŠ ç‰¹å¾é€‰æ‹©å»ºè®®å¤±è´¥: {e}")
    
    def create_feature_importance_dashboard(self, feature_importance: Dict, save_path: str = None) -> str:
        """åˆ›å»ºç‰¹å¾é‡è¦æ€§çœ‹æ¿"""
        try:
            if not feature_importance or 'combined_score' not in feature_importance:
                self.logger.warning("ç¼ºå°‘ç‰¹å¾é‡è¦æ€§æ•°æ®")
                return None
            
            # è·å–æ•°æ®
            combined_scores = feature_importance['combined_score']
            rf_scores = feature_importance.get('random_forest', {})
            mi_scores = feature_importance.get('mutual_info', {})
            correlations = feature_importance.get('correlations', {})
            
            # åˆ›å»ºçœ‹æ¿
            fig = make_subplots(
                rows=2, cols=2,
                subplot_titles=[
                    'ç»¼åˆé‡è¦æ€§æ’åº', 'éšæœºæ£®æ—é‡è¦æ€§',
                    'äº’ä¿¡æ¯é‡è¦æ€§', 'ä¸ç›®æ ‡çš„ç›¸å…³æ€§'
                ],
                specs=[
                    [{"type": "bar"}, {"type": "bar"}],
                    [{"type": "bar"}, {"type": "bar"}]
                ]
            )
            
            # ç»¼åˆé‡è¦æ€§
            top_combined = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:15]
            features, scores = zip(*top_combined)
            
            fig.add_trace(
                go.Bar(x=list(scores), y=list(features), orientation='h',
                      name='ç»¼åˆé‡è¦æ€§', marker_color=self.colors['primary']),
                row=1, col=1
            )
            
            # éšæœºæ£®æ—é‡è¦æ€§
            if rf_scores:
                top_rf = sorted(rf_scores.items(), key=lambda x: x[1], reverse=True)[:15]
                rf_features, rf_vals = zip(*top_rf)
                
                fig.add_trace(
                    go.Bar(x=list(rf_vals), y=list(rf_features), orientation='h',
                          name='éšæœºæ£®æ—', marker_color=self.colors['success']),
                    row=1, col=2
                )
            
            # äº’ä¿¡æ¯é‡è¦æ€§
            if mi_scores:
                top_mi = sorted(mi_scores.items(), key=lambda x: x[1], reverse=True)[:15]
                mi_features, mi_vals = zip(*top_mi)
                
                fig.add_trace(
                    go.Bar(x=list(mi_vals), y=list(mi_features), orientation='h',
                          name='äº’ä¿¡æ¯', marker_color=self.colors['info']),
                    row=2, col=1
                )
            
            # ç›¸å…³æ€§
            if correlations:
                corr_data = [(k, abs(v['correlation'])) for k, v in correlations.items() 
                           if 'correlation' in v]
                if corr_data:
                    top_corr = sorted(corr_data, key=lambda x: x[1], reverse=True)[:15]
                    corr_features, corr_vals = zip(*top_corr)
                    
                    fig.add_trace(
                        go.Bar(x=list(corr_vals), y=list(corr_features), orientation='h',
                              name='ç›¸å…³æ€§', marker_color=self.colors['warning']),
                        row=2, col=2
                    )
            
            fig.update_layout(
                title='ç‰¹å¾é‡è¦æ€§ç»¼åˆåˆ†æçœ‹æ¿',
                height=800,
                showlegend=False
            )
            
            if save_path is None:
                save_path = "results/feature_importance_dashboard.html"
            
            fig.write_html(save_path)
            self.logger.info(f"âœ… ç‰¹å¾é‡è¦æ€§çœ‹æ¿å·²ä¿å­˜: {save_path}")
            
            return save_path
            
        except Exception as e:
            self.logger.exception(f"âŒ åˆ›å»ºç‰¹å¾é‡è¦æ€§çœ‹æ¿å¤±è´¥: {e}")
            return None
    
    def analyze_feature_quality(self, df: pd.DataFrame) -> Dict:
        """åˆ†æç‰¹å¾è´¨é‡"""
        try:
            quality_report = {
                'total_features': 0,
                'high_quality_features': 0,
                'medium_quality_features': 0,
                'low_quality_features': 0,
                'problematic_features': [],
                'recommendations': []
            }
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            quality_report['total_features'] = len(numeric_cols)
            
            for col in numeric_cols:
                feature_quality = self._assess_single_feature_quality(df[col], col)
                
                if feature_quality['overall_score'] >= 0.7:
                    quality_report['high_quality_features'] += 1
                elif feature_quality['overall_score'] >= 0.4:
                    quality_report['medium_quality_features'] += 1
                else:
                    quality_report['low_quality_features'] += 1
                    quality_report['problematic_features'].append({
                        'feature': col,
                        'issues': feature_quality['issues'],
                        'score': feature_quality['overall_score']
                    })
            
            # ç”Ÿæˆå»ºè®®
            if quality_report['low_quality_features'] > quality_report['total_features'] * 0.3:
                quality_report['recommendations'].append("ä½è´¨é‡ç‰¹å¾è¿‡å¤šï¼Œå»ºè®®é‡æ–°è®¾è®¡ç‰¹å¾å·¥ç¨‹")
            
            if len(quality_report['problematic_features']) > 0:
                quality_report['recommendations'].append(f"ç§»é™¤æˆ–ä¿®å¤{len(quality_report['problematic_features'])}ä¸ªé—®é¢˜ç‰¹å¾")
            
            return quality_report
            
        except Exception as e:
            self.logger.exception(f"âŒ ç‰¹å¾è´¨é‡åˆ†æå¤±è´¥: {e}")
            return {}
    
    def _assess_single_feature_quality(self, feature_series: pd.Series, feature_name: str) -> Dict:
        """è¯„ä¼°å•ä¸ªç‰¹å¾çš„è´¨é‡"""
        try:
            quality_assessment = {
                'feature_name': feature_name,
                'overall_score': 0.0,
                'issues': []
            }
            
            scores = []
            
            # 1. ç¼ºå¤±å€¼æ£€æŸ¥
            missing_rate = feature_series.isnull().sum() / len(feature_series)
            if missing_rate < 0.05:
                scores.append(1.0)
            elif missing_rate < 0.2:
                scores.append(0.5)
                quality_assessment['issues'].append(f"ä¸­ç­‰ç¼ºå¤±ç‡: {missing_rate:.2%}")
            else:
                scores.append(0.0)
                quality_assessment['issues'].append(f"é«˜ç¼ºå¤±ç‡: {missing_rate:.2%}")
            
            # 2. æ–¹å·®æ£€æŸ¥
            if feature_series.var() == 0:
                scores.append(0.0)
                quality_assessment['issues'].append("é›¶æ–¹å·®ç‰¹å¾")
            elif feature_series.var() < 1e-8:
                scores.append(0.2)
                quality_assessment['issues'].append("æä½æ–¹å·®")
            else:
                scores.append(1.0)
            
            # 3. å¼‚å¸¸å€¼æ£€æŸ¥
            Q1 = feature_series.quantile(0.25)
            Q3 = feature_series.quantile(0.75)
            IQR = Q3 - Q1
            outlier_rate = len(feature_series[(feature_series < Q1 - 1.5*IQR) | 
                                           (feature_series > Q3 + 1.5*IQR)]) / len(feature_series)
            
            if outlier_rate < 0.05:
                scores.append(1.0)
            elif outlier_rate < 0.1:
                scores.append(0.7)
            else:
                scores.append(0.3)
                quality_assessment['issues'].append(f"é«˜å¼‚å¸¸å€¼ç‡: {outlier_rate:.2%}")
            
            # 4. åˆ†å¸ƒæ£€æŸ¥
            from scipy.stats import skew, kurtosis
            feature_skew = abs(skew(feature_series.dropna()))
            feature_kurt = abs(kurtosis(feature_series.dropna()))
            
            if feature_skew < 2 and feature_kurt < 7:
                scores.append(1.0)
            elif feature_skew < 5 and feature_kurt < 15:
                scores.append(0.6)
            else:
                scores.append(0.2)
                quality_assessment['issues'].append("æç«¯åˆ†å¸ƒååº¦/å³°åº¦")
            
            # è®¡ç®—ç»¼åˆè¯„åˆ†
            quality_assessment['overall_score'] = np.mean(scores)
            
            return quality_assessment
            
        except Exception as e:
            self.logger.error(f"å•ç‰¹å¾è´¨é‡è¯„ä¼°å¤±è´¥: {e}")
            return {'feature_name': feature_name, 'overall_score': 0.0, 'issues': ['è¯„ä¼°å¤±è´¥']}

def main():
    """æµ‹è¯•ç‰¹å¾åˆ†æå™¨åŠŸèƒ½"""
    print("ğŸ“Š æµ‹è¯•ç‰¹å¾åˆ†æå™¨")
    
    analyzer = FeatureAnalyzer()
    print("âœ… ç‰¹å¾åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    print("ğŸ“‹ ä¸»è¦åŠŸèƒ½:")
    print("  - ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–")
    print("  - ç›¸å…³æ€§çƒ­å›¾åˆ†æ")
    print("  - ç‰¹å¾è´¨é‡è¯„ä¼°")
    print("  - ç‰¹å¾é€‰æ‹©å»ºè®®")
    print("  - ç»¼åˆåˆ†ææŠ¥å‘Š")

if __name__ == "__main__":
    main() 