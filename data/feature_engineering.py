"""
ÁâπÂæÅÂ∑•Á®ãÊ®°Âùó - Â¢ûÂº∫ÁâàÊï∞ÊçÆËæìÂÖ•ÂíåÁâπÂæÅÊûÑÈÄ†Á≥ªÁªü
ÂÆûÁé∞Â§öÊó∂Èó¥Â∞∫Â∫¶ÁâπÂæÅ„ÄÅÁâπÂæÅÈáçË¶ÅÊÄßÂàÜÊûê„ÄÅÁâπÂæÅÈÄâÊã©„ÄÅÈôçÁª¥Â§ÑÁêÜÁ≠âÂäüËÉΩ
Ëß£ÂÜ≥31‰∏™ÁâπÂæÅÁöÑ‰ºòÂåñÈóÆÈ¢òÔºåÊèêÂçáÊ®°ÂûãÊïàÊûú
"""
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
import warnings
warnings.filterwarnings('ignore')

from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.inspection import permutation_importance
import talib
from scipy import stats
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt

from utils.config import get_config
from utils.logger import get_logger

class EnhancedFeatureEngineer:
    """Â¢ûÂº∫ÁâàÁâπÂæÅÂ∑•Á®ãÂô®"""
    
    def __init__(self):
        self.config = get_config()
        self.logger = get_logger('FeatureEngineer', 'feature_engineer.log')
        
        # ÁâπÂæÅÈÖçÁΩÆ
        self.feature_config = {
            'enable_multi_timeframe': True,
            'timeframes': ['1h', '4h', '1d'],  # Â§öÊó∂Èó¥Ê°ÜÊû∂
            'enable_volume_features': True,
            'enable_cyclical_features': True,
            'enable_volatility_regime': True,
            'max_features': 50,  # ÊúÄÂ§ßÁâπÂæÅÊï∞ÈôêÂà∂
            'correlation_threshold': 0.9,  # Áõ∏ÂÖ≥ÊÄßÈòàÂÄº
            'importance_threshold': 0.01,  # ÈáçË¶ÅÊÄßÈòàÂÄº
        }
        
        # ÂàùÂßãÂåñscalers
        self.scalers = {
            'standard': StandardScaler(),
            'minmax': MinMaxScaler(),
            'robust': RobustScaler()
        }
        
        # ÁâπÂæÅÈáçË¶ÅÊÄßÂ≠òÂÇ®
        self.feature_importance = {}
        self.feature_correlations = None
        
    def engineer_comprehensive_features(self, df: pd.DataFrame, symbol: str = 'SOLUSDT') -> pd.DataFrame:
        """
        ‚úÖ ÂÖ®Èù¢ÁöÑÁâπÂæÅÂ∑•Á®ã - ‰∏ªË¶ÅÂÖ•Âè£
        Ë°•ÂÖÖÂÖ≥ÈîÆÁâπÂæÅÂπ∂‰ºòÂåñÁé∞ÊúâÁâπÂæÅÈõÜ
        """
        try:
            self.logger.info("üîß ÂºÄÂßãÂÖ®Èù¢ÁâπÂæÅÂ∑•Á®ã...")
            df_enhanced = df.copy()
            
            # 1. Ë°•ÂÖÖÂü∫Á°ÄÊäÄÊúØÊåáÊ†á
            df_enhanced = self._add_missing_technical_indicators(df_enhanced)
            
            # 2. Â¢ûÂº∫Êàê‰∫§ÈáèÁâπÂæÅ
            df_enhanced = self._add_enhanced_volume_features(df_enhanced)
            
            # 3. Â§öÊó∂Èó¥Â∞∫Â∫¶ÁâπÂæÅ
            if self.feature_config['enable_multi_timeframe']:
                df_enhanced = self._add_multi_timeframe_features(df_enhanced)
            
            # 4. Êó∂Èó¥Âë®ÊúüÁâπÂæÅ
            if self.feature_config['enable_cyclical_features']:
                df_enhanced = self._add_cyclical_features(df_enhanced)
            
            # 5. Ê≥¢Âä®ÁéáÂà∂Â∫¶ÁâπÂæÅ
            if self.feature_config['enable_volatility_regime']:
                df_enhanced = self._add_volatility_regime_features(df_enhanced)
            
            # 6. Â∏ÇÂú∫ÂæÆÁªìÊûÑÁâπÂæÅ
            df_enhanced = self._add_microstructure_features(df_enhanced)
            
            # 7. ÁâπÂæÅÊ†áÂáÜÂåñ
            df_enhanced = self._normalize_features(df_enhanced)
            
            self.logger.info(f"‚úÖ ÁâπÂæÅÂ∑•Á®ãÂÆåÊàêÔºåÊÄªÁâπÂæÅÊï∞: {len(df_enhanced.columns)}")
            return df_enhanced
            
        except Exception as e:
            self.logger.exception(f"‚ùå ÁâπÂæÅÂ∑•Á®ãÂ§±Ë¥•: {e}")
            return df
    
    def _add_missing_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ë°•ÂÖÖÁº∫Â§±ÁöÑÊäÄÊúØÊåáÊ†á"""
        try:
            # ‰ª∑Ê†ºÊï∞ÊçÆ
            high = df['high'].values
            low = df['low'].values
            close = df['close'].values
            volume = df['volume'].values
            
            # 1. ATRÁ≥ªÂàóÔºà‰∏çÂêåÂë®ÊúüÔºâ
            df['atr_14'] = talib.ATR(high, low, close, timeperiod=14)
            df['atr_21'] = talib.ATR(high, low, close, timeperiod=21)
            df['atr_ratio'] = df['atr_14'] / df['atr_21']  # ATRÊØîÁéá
            
            # 2. ADXË∂ãÂäøÂº∫Â∫¶Á≥ªÂàó
            df['adx_14'] = talib.ADX(high, low, close, timeperiod=14)
            df['di_plus'] = talib.PLUS_DI(high, low, close, timeperiod=14)
            df['di_minus'] = talib.MINUS_DI(high, low, close, timeperiod=14)
            df['dx'] = talib.DX(high, low, close, timeperiod=14)
            
            # 3. Â∏ÉÊûóÂ∏¶ÂÆåÊï¥Á≥ªÂàó
            bb_upper, bb_middle, bb_lower = talib.BBANDS(close, timeperiod=20, nbdevup=2, nbdevdn=2)
            df['bb_upper'] = bb_upper
            df['bb_middle'] = bb_middle
            df['bb_lower'] = bb_lower
            df['bb_width'] = (bb_upper - bb_lower) / bb_middle  # Â∏¶ÂÆΩ
            df['bb_position'] = (close - bb_lower) / (bb_upper - bb_lower)  # %B‰ΩçÁΩÆ
            
            # 4. ÈöèÊú∫ÊåØËç°Âô®
            df['stoch_k'], df['stoch_d'] = talib.STOCH(high, low, close, 
                                                     fastk_period=14, slowk_period=3, slowd_period=3)
            
            # 5. Â®ÅÂªâÊåáÊ†á
            df['williams_r'] = talib.WILLR(high, low, close, timeperiod=14)
            
            # 6. ÂïÜÂìÅÈÄöÈÅìÊåáÊï∞
            df['cci'] = talib.CCI(high, low, close, timeperiod=14)
            
            # 7. ÊäõÁâ©Á∫øËΩ¨Âêë
            df['sar'] = talib.SAR(high, low, acceleration=0.02, maximum=0.2)
            
            # 8. ÁúüÂÆûÂº∫Â∫¶ÊåáÊï∞
            df['trix'] = talib.TRIX(close, timeperiod=14)
            
            # 9. ËµÑÈáëÊµÅÂêëÊåáÊ†á
            df['mfi'] = talib.MFI(high, low, close, volume, timeperiod=14)
            
            # 10. ÁªàÊûÅÊåØËç°Âô®
            df['ultosc'] = talib.ULTOSC(high, low, close, timeperiod1=7, timeperiod2=14, timeperiod3=28)
            
            self.logger.debug("‚úÖ ÊäÄÊúØÊåáÊ†áË°•ÂÖÖÂÆåÊàê")
            
        except Exception as e:
            self.logger.error(f"‚ùå Ë°•ÂÖÖÊäÄÊúØÊåáÊ†áÂ§±Ë¥•: {e}")
        
        return df
    
    def _add_enhanced_volume_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Â¢ûÂº∫Êàê‰∫§ÈáèÁâπÂæÅ"""
        try:
            volume = df['volume'].values
            close = df['close'].values
            high = df['high'].values
            low = df['low'].values
            
            # 1. Êàê‰∫§ÈáèÁßªÂä®Âπ≥Âùá
            df['volume_sma_10'] = talib.SMA(volume, timeperiod=10)
            df['volume_sma_20'] = talib.SMA(volume, timeperiod=20)
            df['volume_sma_50'] = talib.SMA(volume, timeperiod=50)
            
            # 2. Êàê‰∫§ÈáèÊØîÁéá
            df['volume_ratio_10'] = volume / df['volume_sma_10']
            df['volume_ratio_20'] = volume / df['volume_sma_20']
            
            # 3. Êàê‰∫§ÈáèÂºÇÂ∏∏Ê£ÄÊµã
            volume_std = pd.Series(volume).rolling(20).std()
            volume_mean = pd.Series(volume).rolling(20).mean()
            df['volume_zscore'] = (volume - volume_mean) / volume_std
            df['volume_spike'] = (df['volume_zscore'] > 2).astype(int)
            
            # 4. ‰ª∑ÈáèÂÖ≥Á≥ª
            price_change = np.diff(close, prepend=close[0])
            volume_change = np.diff(volume, prepend=volume[0])
            df['price_volume_corr'] = pd.Series(price_change).rolling(20).corr(pd.Series(volume_change))
            
            # 5. ËµÑÈáëÊµÅÂêë
            typical_price = (high + low + close) / 3
            money_flow = typical_price * volume
            df['money_flow_14'] = pd.Series(money_flow).rolling(14).sum()
            
            # 6. Êàê‰∫§ÈáèÂä†ÊùÉÂπ≥Âùá‰ª∑Ê†º(VWAP)
            df['vwap'] = (typical_price * volume).cumsum() / volume.cumsum()
            df['vwap_ratio'] = close / df['vwap']
            
            # 7. Á¥ØÁßØ/Ê¥æÂèëÁ∫ø
            df['ad_line'] = talib.AD(high, low, close, volume)
            
            # 8. ChaikinËµÑÈáëÊµÅÈáè
            df['chaikin_mf'] = talib.ADOSC(high, low, close, volume, fastperiod=3, slowperiod=10)
            
            self.logger.debug("‚úÖ Êàê‰∫§ÈáèÁâπÂæÅÂ¢ûÂº∫ÂÆåÊàê")
            
        except Exception as e:
            self.logger.error(f"‚ùå Êàê‰∫§ÈáèÁâπÂæÅÂ¢ûÂº∫Â§±Ë¥•: {e}")
        
        return df
    
    def _add_multi_timeframe_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ê∑ªÂä†Â§öÊó∂Èó¥Â∞∫Â∫¶ÁâπÂæÅ"""
        try:
            # Ê®°ÊãüÈ´òÁ∫ßÂà´Êó∂Èó¥Ê°ÜÊû∂ÔºàÂÆûÈôÖÂ∫îËØ•‰ªéAPIËé∑ÂèñÔºâ
            # ËøôÈáåÁî®ÈáçÈááÊ†∑ÊñπÊ≥ïËøë‰ºº
            
            # 1Â∞èÊó∂Á∫ßÂà´ÁâπÂæÅ
            df_1h = df.resample('1H').agg({
                'open': 'first',
                'high': 'max',
                'low': 'min',
                'close': 'last',
                'volume': 'sum'
            }).dropna()
            
            if len(df_1h) > 20:
                # 1Â∞èÊó∂RSI
                rsi_1h = talib.RSI(df_1h['close'].values, timeperiod=14)
                df['rsi_1h'] = self._interpolate_to_original_timeframe(df, df_1h, rsi_1h)
                
                # 1Â∞èÊó∂EMA
                ema_1h = talib.EMA(df_1h['close'].values, timeperiod=21)
                df['ema_1h'] = self._interpolate_to_original_timeframe(df, df_1h, ema_1h)
                
                # 1Â∞èÊó∂Ë∂ãÂäø
                df['trend_1h'] = (df['close'] > df['ema_1h']).astype(int)
            
            # 4Â∞èÊó∂Á∫ßÂà´ÁâπÂæÅ
            df_4h = df.resample('4H').agg({
                'open': 'first',
                'high': 'max',
                'low': 'min',
                'close': 'last',
                'volume': 'sum'
            }).dropna()
            
            if len(df_4h) > 20:
                # 4Â∞èÊó∂MACD
                macd_4h, macd_signal_4h, macd_hist_4h = talib.MACD(df_4h['close'].values)
                df['macd_4h'] = self._interpolate_to_original_timeframe(df, df_4h, macd_4h)
                
                # 4Â∞èÊó∂ADX
                adx_4h = talib.ADX(df_4h['high'].values, df_4h['low'].values, 
                                  df_4h['close'].values, timeperiod=14)
                df['adx_4h'] = self._interpolate_to_original_timeframe(df, df_4h, adx_4h)
            
            # Êó•Á∫øÁ∫ßÂà´ÁâπÂæÅ
            df_1d = df.resample('1D').agg({
                'open': 'first',
                'high': 'max',
                'low': 'min',
                'close': 'last',
                'volume': 'sum'
            }).dropna()
            
            if len(df_1d) > 10:
                # Êó•Á∫øÊîØÊíëÈòªÂäõ‰Ωç
                df['daily_high'] = self._interpolate_to_original_timeframe(df, df_1d, df_1d['high'].values)
                df['daily_low'] = self._interpolate_to_original_timeframe(df, df_1d, df_1d['low'].values)
                
                # Ë∑ùÁ¶ªÊó•Á∫øÈ´ò‰ΩéÁÇπÁöÑË∑ùÁ¶ª
                df['distance_to_daily_high'] = (df['close'] - df['daily_high']) / df['daily_high']
                df['distance_to_daily_low'] = (df['close'] - df['daily_low']) / df['daily_low']
            
            self.logger.debug("‚úÖ Â§öÊó∂Èó¥Â∞∫Â∫¶ÁâπÂæÅÊ∑ªÂä†ÂÆåÊàê")
            
        except Exception as e:
            self.logger.error(f"‚ùå Â§öÊó∂Èó¥Â∞∫Â∫¶ÁâπÂæÅÊ∑ªÂä†Â§±Ë¥•: {e}")
        
        return df
    
    def _interpolate_to_original_timeframe(self, df_original: pd.DataFrame, 
                                         df_resampled: pd.DataFrame, values: np.ndarray) -> pd.Series:
        """Â∞ÜÈ´òÁ∫ßÂà´Êó∂Èó¥Ê°ÜÊû∂ÁöÑÊï∞ÊçÆÊèíÂÄºÂà∞ÂéüÂßãÊó∂Èó¥Ê°ÜÊû∂"""
        try:
            # ÂàõÂª∫‰∏¥Êó∂DataFrameËøõË°åÊèíÂÄº
            temp_df = pd.DataFrame({'values': values}, index=df_resampled.index)
            temp_df = temp_df.reindex(df_original.index, method='ffill')
            return temp_df['values']
        except:
            return pd.Series(np.nan, index=df_original.index)
    
    def _add_cyclical_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ê∑ªÂä†Êó∂Èó¥Âë®ÊúüÁâπÂæÅ"""
        try:
            # ÂÅáËÆæindexÊòØdatetimeÁ±ªÂûã
            if not isinstance(df.index, pd.DatetimeIndex):
                return df
            
            # 1. Â∞èÊó∂ÁâπÂæÅÔºà0-23Ôºâ
            hour = df.index.hour
            df['hour_sin'] = np.sin(2 * np.pi * hour / 24)
            df['hour_cos'] = np.cos(2 * np.pi * hour / 24)
            
            # 2. ÊòüÊúüÁâπÂæÅÔºà0-6Ôºâ
            dayofweek = df.index.dayofweek
            df['dayofweek_sin'] = np.sin(2 * np.pi * dayofweek / 7)
            df['dayofweek_cos'] = np.cos(2 * np.pi * dayofweek / 7)
            
            # 3. Êúà‰ªΩÁâπÂæÅÔºà1-12Ôºâ
            month = df.index.month
            df['month_sin'] = np.sin(2 * np.pi * month / 12)
            df['month_cos'] = np.cos(2 * np.pi * month / 12)
            
            # 4. ‰∫§ÊòìÊó∂ÊÆµÁâπÂæÅ
            # ‰∫öÊ¥≤Êó∂ÊÆµÔºö0-8 UTC
            # Ê¨ßÊ¥≤Êó∂ÊÆµÔºö8-16 UTC
            # ÁæéÊ¥≤Êó∂ÊÆµÔºö16-24 UTC
            df['asia_session'] = ((hour >= 0) & (hour < 8)).astype(int)
            df['europe_session'] = ((hour >= 8) & (hour < 16)).astype(int)
            df['america_session'] = ((hour >= 16) & (hour < 24)).astype(int)
            
            # 5. ÊòØÂê¶Âë®Êú´
            df['is_weekend'] = (dayofweek >= 5).astype(int)
            
            self.logger.debug("‚úÖ Êó∂Èó¥Âë®ÊúüÁâπÂæÅÊ∑ªÂä†ÂÆåÊàê")
            
        except Exception as e:
            self.logger.error(f"‚ùå Êó∂Èó¥Âë®ÊúüÁâπÂæÅÊ∑ªÂä†Â§±Ë¥•: {e}")
        
        return df
    
    def _add_volatility_regime_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ê∑ªÂä†Ê≥¢Âä®ÁéáÂà∂Â∫¶ÁâπÂæÅ"""
        try:
            close = df['close'].values
            
            # 1. Â§öÊúüÈó¥Ê≥¢Âä®Áéá
            returns = np.diff(np.log(close), prepend=np.log(close[0]))
            
            vol_5 = pd.Series(returns).rolling(5).std() * np.sqrt(288)  # 15ÂàÜÈíü -> Âπ¥Âåñ
            vol_20 = pd.Series(returns).rolling(20).std() * np.sqrt(288)
            vol_60 = pd.Series(returns).rolling(60).std() * np.sqrt(288)
            
            df['volatility_5'] = vol_5
            df['volatility_20'] = vol_20
            df['volatility_60'] = vol_60
            
            # 2. Ê≥¢Âä®ÁéáÂà∂Â∫¶ÂàÜÁ±ª
            vol_20_mean = vol_20.rolling(100).mean()
            vol_20_std = vol_20.rolling(100).std()
            
            # ‰ΩéÊ≥¢Âä®ÁéáÂà∂Â∫¶: < mean - 0.5*std
            # ‰∏≠Ê≥¢Âä®ÁéáÂà∂Â∫¶: mean - 0.5*std <= vol <= mean + 0.5*std
            # È´òÊ≥¢Âä®ÁéáÂà∂Â∫¶: > mean + 0.5*std
            low_vol_threshold = vol_20_mean - 0.5 * vol_20_std
            high_vol_threshold = vol_20_mean + 0.5 * vol_20_std
            
            df['vol_regime_low'] = (vol_20 < low_vol_threshold).astype(int)
            df['vol_regime_high'] = (vol_20 > high_vol_threshold).astype(int)
            df['vol_regime_normal'] = ((vol_20 >= low_vol_threshold) & 
                                     (vol_20 <= high_vol_threshold)).astype(int)
            
            # 3. Ê≥¢Âä®ÁéáÂèòÂåñÁéá
            df['volatility_change'] = vol_20.pct_change()
            
            # 4. Ê≥¢Âä®ÁéáÂàÜ‰ΩçÊï∞
            df['volatility_percentile'] = vol_20.rolling(252).rank(pct=True)  # 252‰∏™15ÂàÜÈíüÂë®ÊúüÁ∫¶1‰∏™‰∫§ÊòìÊó•
            
            # 5. GARCHÊ≥¢Âä®ÁéáÔºàÁÆÄÂåñÁâàÔºâ
            squared_returns = returns ** 2
            df['garch_vol'] = pd.Series(squared_returns).ewm(alpha=0.1).mean()
            
            self.logger.debug("‚úÖ Ê≥¢Âä®ÁéáÂà∂Â∫¶ÁâπÂæÅÊ∑ªÂä†ÂÆåÊàê")
            
        except Exception as e:
            self.logger.error(f"‚ùå Ê≥¢Âä®ÁéáÂà∂Â∫¶ÁâπÂæÅÊ∑ªÂä†Â§±Ë¥•: {e}")
        
        return df
    
    def _add_microstructure_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ê∑ªÂä†Â∏ÇÂú∫ÂæÆÁªìÊûÑÁâπÂæÅ"""
        try:
            high = df['high'].values
            low = df['low'].values
            close = df['close'].values
            open_price = df['open'].values
            volume = df['volume'].values
            
            # 1. ‰ª∑Â∑ÆÁâπÂæÅ
            df['spread'] = high - low
            df['spread_pct'] = (high - low) / close
            
            # 2. ÂΩ±Á∫øÂàÜÊûê
            upper_shadow = high - np.maximum(open_price, close)
            lower_shadow = np.minimum(open_price, close) - low
            body_size = np.abs(close - open_price)
            
            df['upper_shadow'] = upper_shadow / close
            df['lower_shadow'] = lower_shadow / close
            df['body_size'] = body_size / close
            df['shadow_ratio'] = (upper_shadow + lower_shadow) / np.maximum(body_size, 1e-8)
            
            # 3. KÁ∫øÂΩ¢ÊÄÅÁâπÂæÅ
            df['doji'] = (body_size / (high - low) < 0.1).astype(int)
            df['hammer'] = ((lower_shadow > 2 * body_size) & (upper_shadow < body_size)).astype(int)
            df['shooting_star'] = ((upper_shadow > 2 * body_size) & (lower_shadow < body_size)).astype(int)
            
            # 4. ‰ª∑Ê†º‰ΩçÁΩÆ
            df['price_position'] = (close - low) / (high - low)
            
            # 5. Êàê‰∫§ÈáèÂº∫Â∫¶
            typical_price = (high + low + close) / 3
            volume_price = volume * typical_price
            df['volume_intensity'] = pd.Series(volume_price).rolling(20).mean()
            
            # 6. ËÆ¢ÂçïÊµÅ‰∏çÂπ≥Ë°°ÔºàËøë‰ººÔºâ
            # Áî®Êàê‰∫§ÈáèÂíå‰ª∑Ê†ºÂèòÂåñËøë‰ºº‰º∞ÁÆó‰π∞ÂçñÂéãÂäõ
            price_change = np.diff(close, prepend=close[0])
            buy_volume = np.where(price_change > 0, volume, 0)
            sell_volume = np.where(price_change < 0, volume, 0)
            
            df['buy_sell_ratio'] = (pd.Series(buy_volume).rolling(20).sum() / 
                                   pd.Series(sell_volume).rolling(20).sum().clip(lower=1))
            
            # 7. ÊµÅÂä®ÊÄßÊåáÊ†á
            df['amihud_illiq'] = np.abs(price_change) / (volume * close)  # AmihudÈùûÊµÅÂä®ÊÄßÊåáÊ†á
            
            self.logger.debug("‚úÖ Â∏ÇÂú∫ÂæÆÁªìÊûÑÁâπÂæÅÊ∑ªÂä†ÂÆåÊàê")
            
        except Exception as e:
            self.logger.error(f"‚ùå Â∏ÇÂú∫ÂæÆÁªìÊûÑÁâπÂæÅÊ∑ªÂä†Â§±Ë¥•: {e}")
        
        return df
    
    def _normalize_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ÁâπÂæÅÊ†áÂáÜÂåñÂ§ÑÁêÜ"""
        try:
            # Ëé∑ÂèñÊï∞ÂÄºÂàó
            numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
            
            # ÊéíÈô§‰∏Ä‰∫õ‰∏çÈúÄË¶ÅÊ†áÂáÜÂåñÁöÑÂàó
            exclude_columns = ['open', 'high', 'low', 'close', 'volume']
            feature_columns = [col for col in numeric_columns if col not in exclude_columns]
            
            for col in feature_columns:
                if col in df.columns:
                    # ‰ΩøÁî®robustÊ†áÂáÜÂåñÂ§ÑÁêÜÂºÇÂ∏∏ÂÄº
                    values = df[col].values.reshape(-1, 1)
                    
                    # ÂÖàÂ§ÑÁêÜÊó†Á©∑Â§ßÂíåNaN
                    finite_mask = np.isfinite(values.flatten())
                    if finite_mask.sum() > 0:
                        # ËÆ°ÁÆóÂàÜ‰ΩçÊï∞ËøõË°åÊà™Â∞æ
                        q1 = np.percentile(values[finite_mask], 1)
                        q99 = np.percentile(values[finite_mask], 99)
                        values = np.clip(values, q1, q99)
                        
                        # Ê†áÂáÜÂåñ
                        scaler = RobustScaler()
                        try:
                            values_scaled = scaler.fit_transform(values)
                            df[col] = values_scaled.flatten()
                        except:
                            # Â¶ÇÊûúÊ†áÂáÜÂåñÂ§±Ë¥•Ôºå‰ΩøÁî®ÁÆÄÂçïÁöÑz-score
                            mean_val = np.nanmean(values)
                            std_val = np.nanstd(values)
                            if std_val > 0:
                                df[col] = (values.flatten() - mean_val) / std_val
            
            # Â§ÑÁêÜÂâ©‰ΩôÁöÑNaN
            df = df.fillna(method='ffill').fillna(0)
            
            self.logger.debug("‚úÖ ÁâπÂæÅÊ†áÂáÜÂåñÂÆåÊàê")
            
        except Exception as e:
            self.logger.error(f"‚ùå ÁâπÂæÅÊ†áÂáÜÂåñÂ§±Ë¥•: {e}")
        
        return df
    
    def analyze_feature_importance(self, df: pd.DataFrame, target_column: str = None) -> Dict:
        """ÂàÜÊûêÁâπÂæÅÈáçË¶ÅÊÄß"""
        try:
            if target_column is None:
                # Â¶ÇÊûúÊ≤°ÊúâÁõÆÊ†áÂàóÔºåÂàõÂª∫‰∏Ä‰∏™Âü∫‰∫éÊú™Êù•Êî∂ÁõäÁöÑÁõÆÊ†á
                target = df['close'].pct_change(5).shift(-5)  # 5ÊúüÂêéÊî∂Áõä
            else:
                target = df[target_column]
            
            # Ëé∑ÂèñÁâπÂæÅÂàó
            feature_columns = df.select_dtypes(include=[np.number]).columns.tolist()
            exclude_columns = ['open', 'high', 'low', 'close', 'volume'] + ([target_column] if target_column else [])
            feature_columns = [col for col in feature_columns if col not in exclude_columns]
            
            # ÂáÜÂ§áÊï∞ÊçÆ
            X = df[feature_columns].fillna(0)
            y = target.fillna(0)
            
            # ÁßªÈô§Êó†ÊïàÊï∞ÊçÆ
            valid_mask = np.isfinite(y) & np.all(np.isfinite(X), axis=1)
            X = X[valid_mask]
            y = y[valid_mask]
            
            if len(X) < 100:
                self.logger.warning("Êï∞ÊçÆÈáè‰∏çË∂≥ÔºåË∑≥ËøáÁâπÂæÅÈáçË¶ÅÊÄßÂàÜÊûê")
                return {}
            
            # 1. Áõ∏ÂÖ≥ÊÄßÂàÜÊûê
            correlations = {}
            for col in feature_columns:
                if col in X.columns:
                    corr, p_value = pearsonr(X[col], y)
                    correlations[col] = {'correlation': corr, 'p_value': p_value}
            
            # 2. ‰∫í‰ø°ÊÅØÂàÜÊûê
            try:
                mi_scores = mutual_info_regression(X, y, random_state=42)
                mi_importance = dict(zip(feature_columns, mi_scores))
            except:
                mi_importance = {}
            
            # 3. ÈöèÊú∫Ê£ÆÊûóÈáçË¶ÅÊÄß
            try:
                rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
                rf.fit(X, y)
                rf_importance = dict(zip(feature_columns, rf.feature_importances_))
            except:
                rf_importance = {}
            
            # ÁªºÂêàÈáçË¶ÅÊÄßËØÑÂàÜ
            importance_summary = {}
            for col in feature_columns:
                score = 0
                if col in correlations:
                    score += abs(correlations[col]['correlation']) * 0.3
                if col in mi_importance:
                    score += mi_importance[col] * 0.3
                if col in rf_importance:
                    score += rf_importance[col] * 0.4
                
                importance_summary[col] = score
            
            # ‰øùÂ≠òÁªìÊûú
            self.feature_importance = {
                'correlations': correlations,
                'mutual_info': mi_importance,
                'random_forest': rf_importance,
                'combined_score': importance_summary
            }
            
            self.logger.info(f"‚úÖ ÁâπÂæÅÈáçË¶ÅÊÄßÂàÜÊûêÂÆåÊàêÔºåÂàÜÊûê‰∫Ü{len(feature_columns)}‰∏™ÁâπÂæÅ")
            return self.feature_importance
            
        except Exception as e:
            self.logger.exception(f"‚ùå ÁâπÂæÅÈáçË¶ÅÊÄßÂàÜÊûêÂ§±Ë¥•: {e}")
            return {}
    
    def select_best_features(self, df: pd.DataFrame, max_features: int = None) -> Tuple[pd.DataFrame, List[str]]:
        """ÈÄâÊã©ÊúÄ‰Ω≥ÁâπÂæÅ"""
        try:
            if not self.feature_importance:
                self.logger.warning("Êú™ËøõË°åÁâπÂæÅÈáçË¶ÅÊÄßÂàÜÊûêÔºåÂÖàÊâßË°åÂàÜÊûê")
                self.analyze_feature_importance(df)
            
            max_features = max_features or self.feature_config['max_features']
            
            # Ëé∑ÂèñÁªºÂêàËØÑÂàÜ
            combined_scores = self.feature_importance.get('combined_score', {})
            
            if not combined_scores:
                self.logger.warning("Êó†Ê≥ïËé∑ÂèñÁâπÂæÅÈáçË¶ÅÊÄßËØÑÂàÜ")
                return df, []
            
            # ÊåâÈáçË¶ÅÊÄßÊéíÂ∫è
            sorted_features = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)
            
            # ÈÄâÊã©topÁâπÂæÅ
            selected_features = [feat for feat, score in sorted_features[:max_features] 
                               if score > self.feature_config['importance_threshold']]
            
            # ‰øùÁïôÂü∫Á°ÄÂàó
            base_columns = ['open', 'high', 'low', 'close', 'volume']
            all_selected = base_columns + selected_features
            
            # ËøáÊª§Â≠òÂú®ÁöÑÂàó
            final_columns = [col for col in all_selected if col in df.columns]
            
            df_selected = df[final_columns].copy()
            
            self.logger.info(f"‚úÖ ÁâπÂæÅÈÄâÊã©ÂÆåÊàêÔºå‰ªé{len(df.columns)}‰∏™ÁâπÂæÅ‰∏≠ÈÄâÊã©‰∫Ü{len(final_columns)}‰∏™")
            
            return df_selected, selected_features
            
        except Exception as e:
            self.logger.exception(f"‚ùå ÁâπÂæÅÈÄâÊã©Â§±Ë¥•: {e}")
            return df, []
    
    def analyze_feature_correlations(self, df: pd.DataFrame) -> pd.DataFrame:
        """ÂàÜÊûêÁâπÂæÅÁõ∏ÂÖ≥ÊÄß"""
        try:
            # Ëé∑ÂèñÊï∞ÂÄºÁâπÂæÅ
            numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
            
            # ËÆ°ÁÆóÁõ∏ÂÖ≥ÊÄßÁü©Èòµ
            correlation_matrix = df[numeric_columns].corr()
            
            # ‰øùÂ≠òÁªìÊûú
            self.feature_correlations = correlation_matrix
            
            # ÊâæÂá∫È´òÁõ∏ÂÖ≥ÊÄßÁâπÂæÅÂØπ
            high_corr_pairs = []
            threshold = self.feature_config['correlation_threshold']
            
            for i in range(len(correlation_matrix.columns)):
                for j in range(i+1, len(correlation_matrix.columns)):
                    corr_value = abs(correlation_matrix.iloc[i, j])
                    if corr_value > threshold:
                        high_corr_pairs.append({
                            'feature1': correlation_matrix.columns[i],
                            'feature2': correlation_matrix.columns[j],
                            'correlation': correlation_matrix.iloc[i, j]
                        })
            
            self.logger.info(f"‚úÖ Áõ∏ÂÖ≥ÊÄßÂàÜÊûêÂÆåÊàêÔºåÂèëÁé∞{len(high_corr_pairs)}ÂØπÈ´òÁõ∏ÂÖ≥ÁâπÂæÅ")
            
            return correlation_matrix
            
        except Exception as e:
            self.logger.exception(f"‚ùå Áõ∏ÂÖ≥ÊÄßÂàÜÊûêÂ§±Ë¥•: {e}")
            return pd.DataFrame()
    
    def remove_highly_correlated_features(self, df: pd.DataFrame, threshold: float = None) -> Tuple[pd.DataFrame, List[str]]:
        """ÁßªÈô§È´òÁõ∏ÂÖ≥ÊÄßÁâπÂæÅ"""
        try:
            threshold = threshold or self.feature_config['correlation_threshold']
            
            if self.feature_correlations is None:
                self.analyze_feature_correlations(df)
            
            # Ëé∑ÂèñÊï∞ÂÄºÁâπÂæÅ
            numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
            base_columns = ['open', 'high', 'low', 'close', 'volume']
            feature_columns = [col for col in numeric_columns if col not in base_columns]
            
            # ÊâæÂá∫Ë¶ÅÁßªÈô§ÁöÑÁâπÂæÅ
            to_remove = set()
            corr_matrix = self.feature_correlations
            
            for i in range(len(feature_columns)):
                for j in range(i+1, len(feature_columns)):
                    col1, col2 = feature_columns[i], feature_columns[j]
                    
                    if col1 in corr_matrix.columns and col2 in corr_matrix.columns:
                        corr_value = abs(corr_matrix.loc[col1, col2])
                        
                        if corr_value > threshold:
                            # ÈÄâÊã©ÈáçË¶ÅÊÄßËæÉ‰ΩéÁöÑÁâπÂæÅÁßªÈô§
                            if self.feature_importance.get('combined_score', {}):
                                score1 = self.feature_importance['combined_score'].get(col1, 0)
                                score2 = self.feature_importance['combined_score'].get(col2, 0)
                                if score1 < score2:
                                    to_remove.add(col1)
                                else:
                                    to_remove.add(col2)
                            else:
                                # Â¶ÇÊûúÊ≤°ÊúâÈáçË¶ÅÊÄß‰ø°ÊÅØÔºåÈöèÊú∫ÁßªÈô§‰∏Ä‰∏™
                                to_remove.add(col2)
            
            # ÁßªÈô§È´òÁõ∏ÂÖ≥ÁâπÂæÅ
            remaining_columns = [col for col in df.columns if col not in to_remove]
            df_filtered = df[remaining_columns].copy()
            
            removed_features = list(to_remove)
            self.logger.info(f"‚úÖ ÁßªÈô§‰∫Ü{len(removed_features)}‰∏™È´òÁõ∏ÂÖ≥ÁâπÂæÅ")
            
            return df_filtered, removed_features
            
        except Exception as e:
            self.logger.exception(f"‚ùå ÁßªÈô§È´òÁõ∏ÂÖ≥ÁâπÂæÅÂ§±Ë¥•: {e}")
            return df, []
    
    def apply_dimensionality_reduction(self, df: pd.DataFrame, n_components: int = None, method: str = 'pca') -> Tuple[pd.DataFrame, Any]:
        """Â∫îÁî®ÈôçÁª¥Â§ÑÁêÜ"""
        try:
            # Ëé∑ÂèñÁâπÂæÅÂàó
            numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
            base_columns = ['open', 'high', 'low', 'close', 'volume']
            feature_columns = [col for col in numeric_columns if col not in base_columns]
            
            X = df[feature_columns].fillna(0)
            n_components = n_components or min(20, len(feature_columns))
            
            if method == 'pca':
                reducer = PCA(n_components=n_components, random_state=42)
                X_reduced = reducer.fit_transform(X)
                
                # ÂàõÂª∫Êñ∞ÁöÑÁâπÂæÅÂêç
                pca_columns = [f'pca_{i+1}' for i in range(n_components)]
                
                # ‰øùÁïôÂü∫Á°ÄÂàóÂíåPCAÁâπÂæÅ
                df_reduced = df[base_columns].copy()
                for i, col in enumerate(pca_columns):
                    df_reduced[col] = X_reduced[:, i]
                
                # ËÆ∞ÂΩïËß£ÈáäÁöÑÊñπÂ∑ÆÊØî‰æã
                explained_variance = reducer.explained_variance_ratio_
                cumulative_variance = np.cumsum(explained_variance)
                
                self.logger.info(f"‚úÖ PCAÈôçÁª¥ÂÆåÊàêÔºå{n_components}‰∏™‰∏ªÊàêÂàÜËß£Èáä‰∫Ü{cumulative_variance[-1]*100:.2f}%ÁöÑÊñπÂ∑Æ")
                
                return df_reduced, {
                    'reducer': reducer,
                    'explained_variance': explained_variance,
                    'cumulative_variance': cumulative_variance,
                    'feature_columns': feature_columns
                }
            
            else:
                self.logger.warning(f"‰∏çÊîØÊåÅÁöÑÈôçÁª¥ÊñπÊ≥ï: {method}")
                return df, None
                
        except Exception as e:
            self.logger.exception(f"‚ùå ÈôçÁª¥Â§ÑÁêÜÂ§±Ë¥•: {e}")
            return df, None
    
    def get_feature_engineering_summary(self, df: pd.DataFrame) -> Dict:
        """Ëé∑ÂèñÁâπÂæÅÂ∑•Á®ãÊëòË¶Å"""
        try:
            summary = {
                'total_features': len(df.columns),
                'numeric_features': len(df.select_dtypes(include=[np.number]).columns),
                'missing_values': df.isnull().sum().sum(),
                'infinite_values': np.isinf(df.select_dtypes(include=[np.number])).sum().sum(),
                'feature_categories': {
                    'base_ohlcv': 5,
                    'technical_indicators': 0,
                    'volume_features': 0,
                    'cyclical_features': 0,
                    'volatility_features': 0,
                    'microstructure_features': 0,
                    'multi_timeframe_features': 0
                }
            }
            
            # ÁªüËÆ°ÁâπÂæÅÁ±ªÂà´
            for col in df.columns:
                if any(indicator in col.lower() for indicator in ['rsi', 'macd', 'ema', 'sma', 'bb', 'atr', 'adx']):
                    summary['feature_categories']['technical_indicators'] += 1
                elif any(vol_term in col.lower() for vol_term in ['volume', 'vwap', 'mfi', 'ad_line']):
                    summary['feature_categories']['volume_features'] += 1
                elif any(time_term in col.lower() for time_term in ['hour', 'day', 'month', 'session']):
                    summary['feature_categories']['cyclical_features'] += 1
                elif any(vol_term in col.lower() for vol_term in ['volatility', 'vol_regime', 'garch']):
                    summary['feature_categories']['volatility_features'] += 1
                elif any(micro_term in col.lower() for micro_term in ['spread', 'shadow', 'doji', 'hammer']):
                    summary['feature_categories']['microstructure_features'] += 1
                elif any(tf_term in col.lower() for tf_term in ['_1h', '_4h', '_1d', 'daily']):
                    summary['feature_categories']['multi_timeframe_features'] += 1
            
            # Ê∑ªÂä†ÈáçË¶ÅÊÄßÂàÜÊûêÁªìÊûú
            if self.feature_importance:
                top_features = sorted(self.feature_importance.get('combined_score', {}).items(), 
                                    key=lambda x: x[1], reverse=True)[:10]
                summary['top_10_features'] = top_features
            
            return summary
            
        except Exception as e:
            self.logger.exception(f"‚ùå Ëé∑ÂèñÁâπÂæÅÂ∑•Á®ãÊëòË¶ÅÂ§±Ë¥•: {e}")
            return {}

def main():
    """ÊµãËØïÁâπÂæÅÂ∑•Á®ãÂäüËÉΩ"""
    print("üîß ÊµãËØïÂ¢ûÂº∫ÁâàÁâπÂæÅÂ∑•Á®ãÁ≥ªÁªü")
    
    # ËøôÈáåÂèØ‰ª•Ê∑ªÂä†ÊµãËØï‰ª£Á†Å
    feature_engineer = EnhancedFeatureEngineer()
    print("‚úÖ ÁâπÂæÅÂ∑•Á®ãÂô®ÂàùÂßãÂåñÂÆåÊàê")
    print("üìã ‰∏ªË¶ÅÂäüËÉΩ:")
    print("  - Â§öÊó∂Èó¥Â∞∫Â∫¶ÁâπÂæÅ")
    print("  - Â¢ûÂº∫ÊäÄÊúØÊåáÊ†á")
    print("  - Êàê‰∫§ÈáèÂàÜÊûêÁâπÂæÅ")
    print("  - Êó∂Èó¥Âë®ÊúüÁâπÂæÅ")
    print("  - Ê≥¢Âä®ÁéáÂà∂Â∫¶ÁâπÂæÅ")
    print("  - ÁâπÂæÅÈáçË¶ÅÊÄßÂàÜÊûê")
    print("  - ÁâπÂæÅÈÄâÊã©ÂíåÈôçÁª¥")

if __name__ == "__main__":
    main() 