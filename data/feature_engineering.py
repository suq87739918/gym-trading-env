"""
ç‰¹å¾å·¥ç¨‹æ¨¡å— - å¢å¼ºç‰ˆæ•°æ®è¾“å…¥å’Œç‰¹å¾æ„é€ ç³»ç»Ÿ
å®ç°å¤šæ—¶é—´å°ºåº¦ç‰¹å¾ã€ç‰¹å¾é‡è¦æ€§åˆ†æã€ç‰¹å¾é€‰æ‹©ã€é™ç»´å¤„ç†ç­‰åŠŸèƒ½
è§£å†³31ä¸ªç‰¹å¾çš„ä¼˜åŒ–é—®é¢˜ï¼Œæå‡æ¨¡å‹æ•ˆæœ
"""
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
import warnings
warnings.filterwarnings('ignore')

from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.inspection import permutation_importance
import talib
from scipy import stats
from scipy.stats import pearsonr
import seaborn as sns
import matplotlib.pyplot as plt

from utils.config import get_config
from utils.logger import get_logger

class EnhancedFeatureEngineer:
    """å¢å¼ºç‰ˆç‰¹å¾å·¥ç¨‹å™¨"""
    
    def __init__(self):
        self.config = get_config()
        self.logger = get_logger('FeatureEngineer', 'feature_engineer.log')
        
        # ç‰¹å¾é…ç½®
        self.feature_config = {
            'enable_multi_timeframe': True,
            'timeframes': ['1h', '4h', '1d'],  # å¤šæ—¶é—´æ¡†æ¶
            'enable_volume_features': True,
            'enable_cyclical_features': True,
            'enable_volatility_regime': True,
            'max_features': 50,  # æœ€å¤§ç‰¹å¾æ•°é™åˆ¶
            'correlation_threshold': 0.9,  # ç›¸å…³æ€§é˜ˆå€¼
            'importance_threshold': 0.01,  # é‡è¦æ€§é˜ˆå€¼
        }
        
        # åˆå§‹åŒ–scalers
        self.scalers = {
            'standard': StandardScaler(),
            'minmax': MinMaxScaler(),
            'robust': RobustScaler()
        }
        
        # ç‰¹å¾é‡è¦æ€§å­˜å‚¨
        self.feature_importance = {}
        self.feature_correlations = None
        
    def engineer_comprehensive_features(self, df: pd.DataFrame, symbol: str = 'SOLUSDT') -> pd.DataFrame:
        """
        âœ… å…¨é¢çš„ç‰¹å¾å·¥ç¨‹ - ä¸»è¦å…¥å£
        è¡¥å……å…³é”®ç‰¹å¾å¹¶ä¼˜åŒ–ç°æœ‰ç‰¹å¾é›†
        """
        try:
            self.logger.info("ğŸ”§ å¼€å§‹å…¨é¢ç‰¹å¾å·¥ç¨‹...")
            df_enhanced = df.copy()
            
            # 1. è¡¥å……åŸºç¡€æŠ€æœ¯æŒ‡æ ‡
            df_enhanced = self._add_missing_technical_indicators(df_enhanced)
            
            # 2. å¢å¼ºæˆäº¤é‡ç‰¹å¾
            df_enhanced = self._add_enhanced_volume_features(df_enhanced)
            
            # 3. å¤šæ—¶é—´å°ºåº¦ç‰¹å¾
            if self.feature_config['enable_multi_timeframe']:
                df_enhanced = self._add_multi_timeframe_features(df_enhanced)
            
            # 4. æ—¶é—´å‘¨æœŸç‰¹å¾
            if self.feature_config['enable_cyclical_features']:
                df_enhanced = self._add_cyclical_features(df_enhanced)
            
            # 5. æ³¢åŠ¨ç‡åˆ¶åº¦ç‰¹å¾
            if self.feature_config['enable_volatility_regime']:
                df_enhanced = self._add_volatility_regime_features(df_enhanced)
            
            # 6. å¸‚åœºå¾®ç»“æ„ç‰¹å¾
            df_enhanced = self._add_microstructure_features(df_enhanced)
            
            # 7. ç‰¹å¾æ ‡å‡†åŒ–
            df_enhanced = self._normalize_features(df_enhanced)
            
            self.logger.info(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œæ€»ç‰¹å¾æ•°: {len(df_enhanced.columns)}")
            return df_enhanced
            
        except Exception as e:
            self.logger.exception(f"âŒ ç‰¹å¾å·¥ç¨‹å¤±è´¥: {e}")
            return df
    
    def _add_missing_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """è¡¥å……ç¼ºå¤±çš„æŠ€æœ¯æŒ‡æ ‡"""
        try:
            # ä»·æ ¼æ•°æ®
            high = df['high'].values
            low = df['low'].values
            close = df['close'].values
            volume = df['volume'].values
            
            # 1. ATRç³»åˆ—ï¼ˆä¸åŒå‘¨æœŸï¼‰
            df['atr_14'] = talib.ATR(high, low, close, timeperiod=14)
            df['atr_21'] = talib.ATR(high, low, close, timeperiod=21)
            df['atr_ratio'] = df['atr_14'] / df['atr_21']  # ATRæ¯”ç‡
            
            # 2. ADXè¶‹åŠ¿å¼ºåº¦ç³»åˆ—
            df['adx_14'] = talib.ADX(high, low, close, timeperiod=14)
            df['di_plus'] = talib.PLUS_DI(high, low, close, timeperiod=14)
            df['di_minus'] = talib.MINUS_DI(high, low, close, timeperiod=14)
            df['dx'] = talib.DX(high, low, close, timeperiod=14)
            
            # 3. å¸ƒæ—å¸¦å®Œæ•´ç³»åˆ—
            bb_upper, bb_middle, bb_lower = talib.BBANDS(close, timeperiod=20, nbdevup=2, nbdevdn=2)
            df['bb_upper'] = bb_upper
            df['bb_middle'] = bb_middle
            df['bb_lower'] = bb_lower
            df['bb_width'] = (bb_upper - bb_lower) / bb_middle  # å¸¦å®½
            df['bb_position'] = (close - bb_lower) / (bb_upper - bb_lower)  # %Bä½ç½®
            
            # 4. éšæœºæŒ¯è¡å™¨
            df['stoch_k'], df['stoch_d'] = talib.STOCH(high, low, close, 
                                                     fastk_period=14, slowk_period=3, slowd_period=3)
            
            # 5. å¨å»‰æŒ‡æ ‡
            df['williams_r'] = talib.WILLR(high, low, close, timeperiod=14)
            
            # 6. å•†å“é€šé“æŒ‡æ•°
            df['cci'] = talib.CCI(high, low, close, timeperiod=14)
            
            # 7. æŠ›ç‰©çº¿è½¬å‘
            df['sar'] = talib.SAR(high, low, acceleration=0.02, maximum=0.2)
            
            # 8. çœŸå®å¼ºåº¦æŒ‡æ•°
            df['trix'] = talib.TRIX(close, timeperiod=14)
            
            # 9. èµ„é‡‘æµå‘æŒ‡æ ‡
            df['mfi'] = talib.MFI(high, low, close, volume, timeperiod=14)
            
            # 10. ç»ˆææŒ¯è¡å™¨
            df['ultosc'] = talib.ULTOSC(high, low, close, timeperiod1=7, timeperiod2=14, timeperiod3=28)
            
            self.logger.debug("âœ… æŠ€æœ¯æŒ‡æ ‡è¡¥å……å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ è¡¥å……æŠ€æœ¯æŒ‡æ ‡å¤±è´¥: {e}")
        
        return df
    
    def _add_enhanced_volume_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¢å¼ºæˆäº¤é‡ç‰¹å¾"""
        try:
            volume = df['volume'].values
            close = df['close'].values
            high = df['high'].values
            low = df['low'].values
            
            # 1. æˆäº¤é‡ç§»åŠ¨å¹³å‡
            df['volume_sma_10'] = talib.SMA(volume, timeperiod=10)
            df['volume_sma_20'] = talib.SMA(volume, timeperiod=20)
            df['volume_sma_50'] = talib.SMA(volume, timeperiod=50)
            
            # 2. æˆäº¤é‡æ¯”ç‡
            df['volume_ratio_10'] = volume / df['volume_sma_10']
            df['volume_ratio_20'] = volume / df['volume_sma_20']
            
            # 3. æˆäº¤é‡å¼‚å¸¸æ£€æµ‹
            volume_std = pd.Series(volume).rolling(20).std()
            volume_mean = pd.Series(volume).rolling(20).mean()
            df['volume_zscore'] = (volume - volume_mean) / volume_std
            df['volume_spike'] = (df['volume_zscore'] > 2).astype(int)
            
            # 4. ä»·é‡å…³ç³»
            price_change = np.diff(close, prepend=close[0])
            volume_change = np.diff(volume, prepend=volume[0])
            df['price_volume_corr'] = pd.Series(price_change).rolling(20).corr(pd.Series(volume_change))
            
            # 5. èµ„é‡‘æµå‘
            typical_price = (high + low + close) / 3
            money_flow = typical_price * volume
            df['money_flow_14'] = pd.Series(money_flow).rolling(14).sum()
            
            # 6. æˆäº¤é‡åŠ æƒå¹³å‡ä»·æ ¼(VWAP)
            df['vwap'] = (typical_price * volume).cumsum() / volume.cumsum()
            df['vwap_ratio'] = close / df['vwap']
            
            # 7. ç´¯ç§¯/æ´¾å‘çº¿
            df['ad_line'] = talib.AD(high, low, close, volume)
            
            # 8. Chaikinèµ„é‡‘æµé‡
            df['chaikin_mf'] = talib.ADOSC(high, low, close, volume, fastperiod=3, slowperiod=10)
            
            self.logger.debug("âœ… æˆäº¤é‡ç‰¹å¾å¢å¼ºå®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ æˆäº¤é‡ç‰¹å¾å¢å¼ºå¤±è´¥: {e}")
        
        return df
    
    def _add_multi_timeframe_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ·»åŠ å¤šæ—¶é—´å°ºåº¦ç‰¹å¾"""
        try:
            # æ¨¡æ‹Ÿé«˜çº§åˆ«æ—¶é—´æ¡†æ¶ï¼ˆå®é™…åº”è¯¥ä»APIè·å–ï¼‰
            # è¿™é‡Œç”¨é‡é‡‡æ ·æ–¹æ³•è¿‘ä¼¼
            
            # 1å°æ—¶çº§åˆ«ç‰¹å¾
            df_1h = df.resample('1H').agg({
                'open': 'first',
                'high': 'max',
                'low': 'min',
                'close': 'last',
                'volume': 'sum'
            }).dropna()
            
            if len(df_1h) > 20:
                # 1å°æ—¶RSI
                rsi_1h = talib.RSI(df_1h['close'].values, timeperiod=14)
                df['rsi_1h'] = self._interpolate_to_original_timeframe(df, df_1h, rsi_1h)
                
                # 1å°æ—¶EMA
                ema_1h = talib.EMA(df_1h['close'].values, timeperiod=21)
                df['ema_1h'] = self._interpolate_to_original_timeframe(df, df_1h, ema_1h)
                
                # 1å°æ—¶è¶‹åŠ¿
                df['trend_1h'] = (df['close'] > df['ema_1h']).astype(int)
            
            # 4å°æ—¶çº§åˆ«ç‰¹å¾
            df_4h = df.resample('4H').agg({
                'open': 'first',
                'high': 'max',
                'low': 'min',
                'close': 'last',
                'volume': 'sum'
            }).dropna()
            
            if len(df_4h) > 20:
                # 4å°æ—¶MACD
                macd_4h, macd_signal_4h, macd_hist_4h = talib.MACD(df_4h['close'].values)
                df['macd_4h'] = self._interpolate_to_original_timeframe(df, df_4h, macd_4h)
                
                # 4å°æ—¶ADX
                adx_4h = talib.ADX(df_4h['high'].values, df_4h['low'].values, 
                                  df_4h['close'].values, timeperiod=14)
                df['adx_4h'] = self._interpolate_to_original_timeframe(df, df_4h, adx_4h)
            
            # æ—¥çº¿çº§åˆ«ç‰¹å¾
            df_1d = df.resample('1D').agg({
                'open': 'first',
                'high': 'max',
                'low': 'min',
                'close': 'last',
                'volume': 'sum'
            }).dropna()
            
            if len(df_1d) > 10:
                # æ—¥çº¿æ”¯æ’‘é˜»åŠ›ä½
                df['daily_high'] = self._interpolate_to_original_timeframe(df, df_1d, df_1d['high'].values)
                df['daily_low'] = self._interpolate_to_original_timeframe(df, df_1d, df_1d['low'].values)
                
                # è·ç¦»æ—¥çº¿é«˜ä½ç‚¹çš„è·ç¦»
                df['distance_to_daily_high'] = (df['close'] - df['daily_high']) / df['daily_high']
                df['distance_to_daily_low'] = (df['close'] - df['daily_low']) / df['daily_low']
            
            self.logger.debug("âœ… å¤šæ—¶é—´å°ºåº¦ç‰¹å¾æ·»åŠ å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ å¤šæ—¶é—´å°ºåº¦ç‰¹å¾æ·»åŠ å¤±è´¥: {e}")
        
        return df
    
    def _interpolate_to_original_timeframe(self, df_original: pd.DataFrame, 
                                         df_resampled: pd.DataFrame, values: np.ndarray) -> pd.Series:
        """å°†é«˜çº§åˆ«æ—¶é—´æ¡†æ¶çš„æ•°æ®æ’å€¼åˆ°åŸå§‹æ—¶é—´æ¡†æ¶"""
        try:
            # åˆ›å»ºä¸´æ—¶DataFrameè¿›è¡Œæ’å€¼
            temp_df = pd.DataFrame({'values': values}, index=df_resampled.index)
            temp_df = temp_df.reindex(df_original.index, method='ffill')
            return temp_df['values']
        except:
            return pd.Series(np.nan, index=df_original.index)
    
    def _add_cyclical_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ·»åŠ æ—¶é—´å‘¨æœŸç‰¹å¾"""
        try:
            # å‡è®¾indexæ˜¯datetimeç±»å‹
            if not isinstance(df.index, pd.DatetimeIndex):
                return df
            
            # 1. å°æ—¶ç‰¹å¾ï¼ˆ0-23ï¼‰
            hour = df.index.hour
            df['hour_sin'] = np.sin(2 * np.pi * hour / 24)
            df['hour_cos'] = np.cos(2 * np.pi * hour / 24)
            
            # 2. æ˜ŸæœŸç‰¹å¾ï¼ˆ0-6ï¼‰
            dayofweek = df.index.dayofweek
            df['dayofweek_sin'] = np.sin(2 * np.pi * dayofweek / 7)
            df['dayofweek_cos'] = np.cos(2 * np.pi * dayofweek / 7)
            
            # 3. æœˆä»½ç‰¹å¾ï¼ˆ1-12ï¼‰
            month = df.index.month
            df['month_sin'] = np.sin(2 * np.pi * month / 12)
            df['month_cos'] = np.cos(2 * np.pi * month / 12)
            
            # 4. äº¤æ˜“æ—¶æ®µç‰¹å¾
            # äºšæ´²æ—¶æ®µï¼š0-8 UTC
            # æ¬§æ´²æ—¶æ®µï¼š8-16 UTC
            # ç¾æ´²æ—¶æ®µï¼š16-24 UTC
            df['asia_session'] = ((hour >= 0) & (hour < 8)).astype(int)
            df['europe_session'] = ((hour >= 8) & (hour < 16)).astype(int)
            df['america_session'] = ((hour >= 16) & (hour < 24)).astype(int)
            
            # 5. æ˜¯å¦å‘¨æœ«
            df['is_weekend'] = (dayofweek >= 5).astype(int)
            
            self.logger.debug("âœ… æ—¶é—´å‘¨æœŸç‰¹å¾æ·»åŠ å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ æ—¶é—´å‘¨æœŸç‰¹å¾æ·»åŠ å¤±è´¥: {e}")
        
        return df
    
    def _add_volatility_regime_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ·»åŠ æ³¢åŠ¨ç‡åˆ¶åº¦ç‰¹å¾"""
        try:
            close = df['close'].values
            
            # 1. å¤šæœŸé—´æ³¢åŠ¨ç‡
            returns = np.diff(np.log(close), prepend=np.log(close[0]))
            
            vol_5 = pd.Series(returns).rolling(5).std() * np.sqrt(288)  # 15åˆ†é’Ÿ -> å¹´åŒ–
            vol_20 = pd.Series(returns).rolling(20).std() * np.sqrt(288)
            vol_60 = pd.Series(returns).rolling(60).std() * np.sqrt(288)
            
            df['volatility_5'] = vol_5
            df['volatility_20'] = vol_20
            df['volatility_60'] = vol_60
            
            # 2. æ³¢åŠ¨ç‡åˆ¶åº¦åˆ†ç±»
            vol_20_mean = vol_20.rolling(100).mean()
            vol_20_std = vol_20.rolling(100).std()
            
            # ä½æ³¢åŠ¨ç‡åˆ¶åº¦: < mean - 0.5*std
            # ä¸­æ³¢åŠ¨ç‡åˆ¶åº¦: mean - 0.5*std <= vol <= mean + 0.5*std
            # é«˜æ³¢åŠ¨ç‡åˆ¶åº¦: > mean + 0.5*std
            low_vol_threshold = vol_20_mean - 0.5 * vol_20_std
            high_vol_threshold = vol_20_mean + 0.5 * vol_20_std
            
            df['vol_regime_low'] = (vol_20 < low_vol_threshold).astype(int)
            df['vol_regime_high'] = (vol_20 > high_vol_threshold).astype(int)
            df['vol_regime_normal'] = ((vol_20 >= low_vol_threshold) & 
                                     (vol_20 <= high_vol_threshold)).astype(int)
            
            # 3. æ³¢åŠ¨ç‡å˜åŒ–ç‡
            df['volatility_change'] = vol_20.pct_change()
            
            # 4. æ³¢åŠ¨ç‡åˆ†ä½æ•°
            df['volatility_percentile'] = vol_20.rolling(252).rank(pct=True)  # 252ä¸ª15åˆ†é’Ÿå‘¨æœŸçº¦1ä¸ªäº¤æ˜“æ—¥
            
            # 5. GARCHæ³¢åŠ¨ç‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
            squared_returns = returns ** 2
            df['garch_vol'] = pd.Series(squared_returns).ewm(alpha=0.1).mean()
            
            self.logger.debug("âœ… æ³¢åŠ¨ç‡åˆ¶åº¦ç‰¹å¾æ·»åŠ å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ æ³¢åŠ¨ç‡åˆ¶åº¦ç‰¹å¾æ·»åŠ å¤±è´¥: {e}")
        
        return df
    
    def _add_microstructure_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ·»åŠ å¸‚åœºå¾®ç»“æ„ç‰¹å¾"""
        try:
            high = df['high'].values
            low = df['low'].values
            close = df['close'].values
            open_price = df['open'].values
            volume = df['volume'].values
            
            # 1. ä»·å·®ç‰¹å¾
            df['spread'] = high - low
            df['spread_pct'] = (high - low) / close
            
            # 2. å½±çº¿åˆ†æ
            upper_shadow = high - np.maximum(open_price, close)
            lower_shadow = np.minimum(open_price, close) - low
            body_size = np.abs(close - open_price)
            
            df['upper_shadow'] = upper_shadow / close
            df['lower_shadow'] = lower_shadow / close
            df['body_size'] = body_size / close
            df['shadow_ratio'] = (upper_shadow + lower_shadow) / np.maximum(body_size, 1e-8)
            
            # 3. Kçº¿å½¢æ€ç‰¹å¾
            df['doji'] = (body_size / (high - low) < 0.1).astype(int)
            df['hammer'] = ((lower_shadow > 2 * body_size) & (upper_shadow < body_size)).astype(int)
            df['shooting_star'] = ((upper_shadow > 2 * body_size) & (lower_shadow < body_size)).astype(int)
            
            # 4. ä»·æ ¼ä½ç½®
            df['price_position'] = (close - low) / (high - low)
            
            # 5. æˆäº¤é‡å¼ºåº¦
            typical_price = (high + low + close) / 3
            volume_price = volume * typical_price
            df['volume_intensity'] = pd.Series(volume_price).rolling(20).mean()
            
            # 6. è®¢å•æµä¸å¹³è¡¡ï¼ˆè¿‘ä¼¼ï¼‰
            # ç”¨æˆäº¤é‡å’Œä»·æ ¼å˜åŒ–è¿‘ä¼¼ä¼°ç®—ä¹°å–å‹åŠ›
            price_change = np.diff(close, prepend=close[0])
            buy_volume = np.where(price_change > 0, volume, 0)
            sell_volume = np.where(price_change < 0, volume, 0)
            
            df['buy_sell_ratio'] = (pd.Series(buy_volume).rolling(20).sum() / 
                                   pd.Series(sell_volume).rolling(20).sum().clip(lower=1))
            
            # 7. æµåŠ¨æ€§æŒ‡æ ‡
            df['amihud_illiq'] = np.abs(price_change) / (volume * close)  # AmihudéæµåŠ¨æ€§æŒ‡æ ‡
            
            self.logger.debug("âœ… å¸‚åœºå¾®ç»“æ„ç‰¹å¾æ·»åŠ å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ å¸‚åœºå¾®ç»“æ„ç‰¹å¾æ·»åŠ å¤±è´¥: {e}")
        
        return df
    
    def _normalize_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç‰¹å¾æ ‡å‡†åŒ–å¤„ç†"""
        try:
            # è·å–æ•°å€¼åˆ—
            numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
            
            # æ’é™¤ä¸€äº›ä¸éœ€è¦æ ‡å‡†åŒ–çš„åˆ—
            exclude_columns = ['open', 'high', 'low', 'close', 'volume']
            feature_columns = [col for col in numeric_columns if col not in exclude_columns]
            
            for col in feature_columns:
                if col in df.columns:
                    # ä½¿ç”¨robustæ ‡å‡†åŒ–å¤„ç†å¼‚å¸¸å€¼
                    values = df[col].values.reshape(-1, 1)
                    
                    # å…ˆå¤„ç†æ— ç©·å¤§å’ŒNaN
                    finite_mask = np.isfinite(values.flatten())
                    if finite_mask.sum() > 0:
                        # è®¡ç®—åˆ†ä½æ•°è¿›è¡Œæˆªå°¾
                        q1 = np.percentile(values[finite_mask], 1)
                        q99 = np.percentile(values[finite_mask], 99)
                        values = np.clip(values, q1, q99)
                        
                        # æ ‡å‡†åŒ–
                        scaler = RobustScaler()
                        try:
                            values_scaled = scaler.fit_transform(values)
                            df[col] = values_scaled.flatten()
                        except:
                            # å¦‚æœæ ‡å‡†åŒ–å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„z-score
                            mean_val = np.nanmean(values)
                            std_val = np.nanstd(values)
                            if std_val > 0:
                                df[col] = (values.flatten() - mean_val) / std_val
            
            # å¤„ç†å‰©ä½™çš„NaN
            df = df.fillna(method='ffill').fillna(0)
            
            self.logger.debug("âœ… ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ ç‰¹å¾æ ‡å‡†åŒ–å¤±è´¥: {e}")
        
        return df
    
    def analyze_feature_importance(self, df: pd.DataFrame, target_column: str = None) -> Dict:
        """åˆ†æç‰¹å¾é‡è¦æ€§"""
        try:
            if target_column is None:
                # å¦‚æœæ²¡æœ‰ç›®æ ‡åˆ—ï¼Œåˆ›å»ºä¸€ä¸ªåŸºäºæœªæ¥æ”¶ç›Šçš„ç›®æ ‡
                target = df['close'].pct_change(5).shift(-5)  # 5æœŸåæ”¶ç›Š
            else:
                target = df[target_column]
            
            # è·å–ç‰¹å¾åˆ—
            feature_columns = df.select_dtypes(include=[np.number]).columns.tolist()
            exclude_columns = ['open', 'high', 'low', 'close', 'volume'] + ([target_column] if target_column else [])
            feature_columns = [col for col in feature_columns if col not in exclude_columns]
            
            # å‡†å¤‡æ•°æ®
            X = df[feature_columns].fillna(0)
            y = target.fillna(0)
            
            # ç§»é™¤æ— æ•ˆæ•°æ®
            valid_mask = np.isfinite(y) & np.all(np.isfinite(X), axis=1)
            X = X[valid_mask]
            y = y[valid_mask]
            
            if len(X) < 100:
                self.logger.warning("æ•°æ®é‡ä¸è¶³ï¼Œè·³è¿‡ç‰¹å¾é‡è¦æ€§åˆ†æ")
                return {}
            
            # 1. ç›¸å…³æ€§åˆ†æ
            correlations = {}
            for col in feature_columns:
                if col in X.columns:
                    corr, p_value = pearsonr(X[col], y)
                    correlations[col] = {'correlation': corr, 'p_value': p_value}
            
            # 2. äº’ä¿¡æ¯åˆ†æ
            try:
                mi_scores = mutual_info_regression(X, y, random_state=42)
                mi_importance = dict(zip(feature_columns, mi_scores))
            except:
                mi_importance = {}
            
            # 3. éšæœºæ£®æ—é‡è¦æ€§
            try:
                rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
                rf.fit(X, y)
                rf_importance = dict(zip(feature_columns, rf.feature_importances_))
            except:
                rf_importance = {}
            
            # ç»¼åˆé‡è¦æ€§è¯„åˆ†
            importance_summary = {}
            for col in feature_columns:
                score = 0
                if col in correlations:
                    score += abs(correlations[col]['correlation']) * 0.3
                if col in mi_importance:
                    score += mi_importance[col] * 0.3
                if col in rf_importance:
                    score += rf_importance[col] * 0.4
                
                importance_summary[col] = score
            
            # ä¿å­˜ç»“æœ
            self.feature_importance = {
                'correlations': correlations,
                'mutual_info': mi_importance,
                'random_forest': rf_importance,
                'combined_score': importance_summary
            }
            
            self.logger.info(f"âœ… ç‰¹å¾é‡è¦æ€§åˆ†æå®Œæˆï¼Œåˆ†æäº†{len(feature_columns)}ä¸ªç‰¹å¾")
            return self.feature_importance
            
        except Exception as e:
            self.logger.exception(f"âŒ ç‰¹å¾é‡è¦æ€§åˆ†æå¤±è´¥: {e}")
            return {}
    
    def select_best_features(self, df: pd.DataFrame, max_features: int = None) -> Tuple[pd.DataFrame, List[str]]:
        """é€‰æ‹©æœ€ä½³ç‰¹å¾"""
        try:
            if not self.feature_importance:
                self.logger.warning("æœªè¿›è¡Œç‰¹å¾é‡è¦æ€§åˆ†æï¼Œå…ˆæ‰§è¡Œåˆ†æ")
                self.analyze_feature_importance(df)
            
            max_features = max_features or self.feature_config['max_features']
            
            # è·å–ç»¼åˆè¯„åˆ†
            combined_scores = self.feature_importance.get('combined_score', {})
            
            if not combined_scores:
                self.logger.warning("æ— æ³•è·å–ç‰¹å¾é‡è¦æ€§è¯„åˆ†")
                return df, []
            
            # æŒ‰é‡è¦æ€§æ’åº
            sorted_features = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)
            
            # é€‰æ‹©topç‰¹å¾
            selected_features = [feat for feat, score in sorted_features[:max_features] 
                               if score > self.feature_config['importance_threshold']]
            
            # ä¿ç•™åŸºç¡€åˆ—
            base_columns = ['open', 'high', 'low', 'close', 'volume']
            all_selected = base_columns + selected_features
            
            # è¿‡æ»¤å­˜åœ¨çš„åˆ—
            final_columns = [col for col in all_selected if col in df.columns]
            
            df_selected = df[final_columns].copy()
            
            self.logger.info(f"âœ… ç‰¹å¾é€‰æ‹©å®Œæˆï¼Œä»{len(df.columns)}ä¸ªç‰¹å¾ä¸­é€‰æ‹©äº†{len(final_columns)}ä¸ª")
            
            return df_selected, selected_features
            
        except Exception as e:
            self.logger.exception(f"âŒ ç‰¹å¾é€‰æ‹©å¤±è´¥: {e}")
            return df, []
    
    def analyze_feature_correlations(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ†æç‰¹å¾ç›¸å…³æ€§"""
        try:
            # è·å–æ•°å€¼ç‰¹å¾
            numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
            
            # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
            correlation_matrix = df[numeric_columns].corr()
            
            # ä¿å­˜ç»“æœ
            self.feature_correlations = correlation_matrix
            
            # æ‰¾å‡ºé«˜ç›¸å…³æ€§ç‰¹å¾å¯¹
            high_corr_pairs = []
            threshold = self.feature_config['correlation_threshold']
            
            for i in range(len(correlation_matrix.columns)):
                for j in range(i+1, len(correlation_matrix.columns)):
                    corr_value = abs(correlation_matrix.iloc[i, j])
                    if corr_value > threshold:
                        high_corr_pairs.append({
                            'feature1': correlation_matrix.columns[i],
                            'feature2': correlation_matrix.columns[j],
                            'correlation': correlation_matrix.iloc[i, j]
                        })
            
            self.logger.info(f"âœ… ç›¸å…³æ€§åˆ†æå®Œæˆï¼Œå‘ç°{len(high_corr_pairs)}å¯¹é«˜ç›¸å…³ç‰¹å¾")
            
            return correlation_matrix
            
        except Exception as e:
            self.logger.exception(f"âŒ ç›¸å…³æ€§åˆ†æå¤±è´¥: {e}")
            return pd.DataFrame()
    
    def remove_highly_correlated_features(self, df: pd.DataFrame, threshold: float = None) -> Tuple[pd.DataFrame, List[str]]:
        """ç§»é™¤é«˜ç›¸å…³æ€§ç‰¹å¾"""
        try:
            threshold = threshold or self.feature_config['correlation_threshold']
            
            if self.feature_correlations is None:
                self.analyze_feature_correlations(df)
            
            # è·å–æ•°å€¼ç‰¹å¾
            numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
            base_columns = ['open', 'high', 'low', 'close', 'volume']
            feature_columns = [col for col in numeric_columns if col not in base_columns]
            
            # æ‰¾å‡ºè¦ç§»é™¤çš„ç‰¹å¾
            to_remove = set()
            corr_matrix = self.feature_correlations
            
            for i in range(len(feature_columns)):
                for j in range(i+1, len(feature_columns)):
                    col1, col2 = feature_columns[i], feature_columns[j]
                    
                    if col1 in corr_matrix.columns and col2 in corr_matrix.columns:
                        corr_value = abs(corr_matrix.loc[col1, col2])
                        
                        if corr_value > threshold:
                            # é€‰æ‹©é‡è¦æ€§è¾ƒä½çš„ç‰¹å¾ç§»é™¤
                            if self.feature_importance.get('combined_score', {}):
                                score1 = self.feature_importance['combined_score'].get(col1, 0)
                                score2 = self.feature_importance['combined_score'].get(col2, 0)
                                if score1 < score2:
                                    to_remove.add(col1)
                                else:
                                    to_remove.add(col2)
                            else:
                                # å¦‚æœæ²¡æœ‰é‡è¦æ€§ä¿¡æ¯ï¼Œéšæœºç§»é™¤ä¸€ä¸ª
                                to_remove.add(col2)
            
            # ç§»é™¤é«˜ç›¸å…³ç‰¹å¾
            remaining_columns = [col for col in df.columns if col not in to_remove]
            df_filtered = df[remaining_columns].copy()
            
            removed_features = list(to_remove)
            self.logger.info(f"âœ… ç§»é™¤äº†{len(removed_features)}ä¸ªé«˜ç›¸å…³ç‰¹å¾")
            
            return df_filtered, removed_features
            
        except Exception as e:
            self.logger.exception(f"âŒ ç§»é™¤é«˜ç›¸å…³ç‰¹å¾å¤±è´¥: {e}")
            return df, []
    
    def apply_dimensionality_reduction(self, df: pd.DataFrame, n_components: int = None, method: str = 'pca') -> Tuple[pd.DataFrame, Any]:
        """åº”ç”¨é™ç»´å¤„ç†"""
        try:
            # è·å–ç‰¹å¾åˆ—
            numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
            base_columns = ['open', 'high', 'low', 'close', 'volume']
            feature_columns = [col for col in numeric_columns if col not in base_columns]
            
            X = df[feature_columns].fillna(0)
            n_components = n_components or min(20, len(feature_columns))
            
            if method == 'pca':
                reducer = PCA(n_components=n_components, random_state=42)
                X_reduced = reducer.fit_transform(X)
                
                # åˆ›å»ºæ–°çš„ç‰¹å¾å
                pca_columns = [f'pca_{i+1}' for i in range(n_components)]
                
                # ä¿ç•™åŸºç¡€åˆ—å’ŒPCAç‰¹å¾
                df_reduced = df[base_columns].copy()
                for i, col in enumerate(pca_columns):
                    df_reduced[col] = X_reduced[:, i]
                
                # è®°å½•è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹
                explained_variance = reducer.explained_variance_ratio_
                cumulative_variance = np.cumsum(explained_variance)
                
                self.logger.info(f"âœ… PCAé™ç»´å®Œæˆï¼Œ{n_components}ä¸ªä¸»æˆåˆ†è§£é‡Šäº†{cumulative_variance[-1]*100:.2f}%çš„æ–¹å·®")
                
                return df_reduced, {
                    'reducer': reducer,
                    'explained_variance': explained_variance,
                    'cumulative_variance': cumulative_variance,
                    'feature_columns': feature_columns
                }
            
            else:
                self.logger.warning(f"ä¸æ”¯æŒçš„é™ç»´æ–¹æ³•: {method}")
                return df, None
                
        except Exception as e:
            self.logger.exception(f"âŒ é™ç»´å¤„ç†å¤±è´¥: {e}")
            return df, None
    
    def get_feature_engineering_summary(self, df: pd.DataFrame) -> Dict:
        """è·å–ç‰¹å¾å·¥ç¨‹æ‘˜è¦"""
        try:
            summary = {
                'total_features': len(df.columns),
                'numeric_features': len(df.select_dtypes(include=[np.number]).columns),
                'missing_values': df.isnull().sum().sum(),
                'infinite_values': np.isinf(df.select_dtypes(include=[np.number])).sum().sum(),
                'feature_categories': {
                    'base_ohlcv': 5,
                    'technical_indicators': 0,
                    'volume_features': 0,
                    'cyclical_features': 0,
                    'volatility_features': 0,
                    'microstructure_features': 0,
                    'multi_timeframe_features': 0
                }
            }
            
            # ç»Ÿè®¡ç‰¹å¾ç±»åˆ«
            for col in df.columns:
                if any(indicator in col.lower() for indicator in ['rsi', 'macd', 'ema', 'sma', 'bb', 'atr', 'adx']):
                    summary['feature_categories']['technical_indicators'] += 1
                elif any(vol_term in col.lower() for vol_term in ['volume', 'vwap', 'mfi', 'ad_line']):
                    summary['feature_categories']['volume_features'] += 1
                elif any(time_term in col.lower() for time_term in ['hour', 'day', 'month', 'session']):
                    summary['feature_categories']['cyclical_features'] += 1
                elif any(vol_term in col.lower() for vol_term in ['volatility', 'vol_regime', 'garch']):
                    summary['feature_categories']['volatility_features'] += 1
                elif any(micro_term in col.lower() for micro_term in ['spread', 'shadow', 'doji', 'hammer']):
                    summary['feature_categories']['microstructure_features'] += 1
                elif any(tf_term in col.lower() for tf_term in ['_1h', '_4h', '_1d', 'daily']):
                    summary['feature_categories']['multi_timeframe_features'] += 1
            
            # æ·»åŠ é‡è¦æ€§åˆ†æç»“æœ
            if self.feature_importance:
                top_features = sorted(self.feature_importance.get('combined_score', {}).items(), 
                                    key=lambda x: x[1], reverse=True)[:10]
                summary['top_10_features'] = top_features
            
            return summary
            
        except Exception as e:
            self.logger.exception(f"âŒ è·å–ç‰¹å¾å·¥ç¨‹æ‘˜è¦å¤±è´¥: {e}")
            return {}

def main():
    """æµ‹è¯•ç‰¹å¾å·¥ç¨‹åŠŸèƒ½"""
    print("ğŸ”§ æµ‹è¯•å¢å¼ºç‰ˆç‰¹å¾å·¥ç¨‹ç³»ç»Ÿ")
    
    # è¿™é‡Œå¯ä»¥æ·»åŠ æµ‹è¯•ä»£ç 
    feature_engineer = EnhancedFeatureEngineer()
    print("âœ… ç‰¹å¾å·¥ç¨‹å™¨åˆå§‹åŒ–å®Œæˆ")
    print("ğŸ“‹ ä¸»è¦åŠŸèƒ½:")
    print("  - å¤šæ—¶é—´å°ºåº¦ç‰¹å¾")
    print("  - å¢å¼ºæŠ€æœ¯æŒ‡æ ‡")
    print("  - æˆäº¤é‡åˆ†æç‰¹å¾")
    print("  - æ—¶é—´å‘¨æœŸç‰¹å¾")
    print("  - æ³¢åŠ¨ç‡åˆ¶åº¦ç‰¹å¾")
    print("  - ç‰¹å¾é‡è¦æ€§åˆ†æ")
    print("  - ç‰¹å¾é€‰æ‹©å’Œé™ç»´")

if __name__ == "__main__":
    main() 